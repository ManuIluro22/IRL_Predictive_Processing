{
 "cells": [
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "import numpy as np"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-14T00:33:15.153720100Z",
     "start_time": "2024-06-14T00:33:11.568746400Z"
    }
   },
   "id": "3ab83f0f76db1eec",
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "%matplotlib inline\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-14T00:33:15.210365800Z",
     "start_time": "2024-06-14T00:33:15.159226200Z"
    }
   },
   "id": "e9d67603e258db1b",
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "RETOS_BEBRASK_dataset = pd.read_excel(\"RETOS_BEBRASK_long.xlsx\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-14T00:33:16.314218800Z",
     "start_time": "2024-06-14T00:33:15.189106800Z"
    }
   },
   "id": "65455679049021d4",
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "rating_columns = [col for col in RETOS_BEBRASK_dataset.columns if 'Rating0' in col]\n",
    "fulfilled_columns = [col for col in RETOS_BEBRASK_dataset.columns if 'Fulfilled' in col]\n",
    "actions_dataset = RETOS_BEBRASK_dataset[rating_columns]-1\n",
    "states_dataset = RETOS_BEBRASK_dataset[fulfilled_columns]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-14T00:33:16.332254300Z",
     "start_time": "2024-06-14T00:33:16.319720300Z"
    }
   },
   "id": "498ce44c9da5a423",
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Assuming data for each subject is grouped together\n",
    "batches_per_subject = 5\n",
    "\n",
    "# Dictionary to hold training and testing data for cross-validation\n",
    "cross_val_data = {}\n",
    "\n",
    "# Use enumerate to get index and subject data from actions and states datasets\n",
    "for i, (actions, states) in enumerate(zip(actions_dataset.values, states_dataset.values)):\n",
    "    num_actions = len(actions)\n",
    "    batch_size = num_actions // batches_per_subject\n",
    "    cross_val_data[i] = []\n",
    "\n",
    "    for j in range(batches_per_subject):\n",
    "        start_index = j * batch_size\n",
    "        if j == batches_per_subject - 1:\n",
    "            end_index = num_actions  # Ensure the last batch goes up to the end\n",
    "        else:\n",
    "            end_index = start_index + batch_size\n",
    "        \n",
    "        # Test data for the current fold\n",
    "        test_data = (states[start_index:end_index], actions[start_index:end_index])\n",
    "\n",
    "        # Training data for the current fold\n",
    "        # Combine slices before and after the test segment\n",
    "        train_states = np.concatenate((states[:start_index], states[end_index:]))\n",
    "        train_actions = np.concatenate((actions[:start_index], actions[end_index:]))\n",
    "        train_data = (train_states, train_actions)\n",
    "\n",
    "        # Save the train and test data in the dictionary\n",
    "        cross_val_data[i].append((train_data, test_data))\n",
    "\n",
    "# Now, `cross_val_data` is ready for use in your training/testing loops\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-14T00:33:16.371658700Z",
     "start_time": "2024-06-14T00:33:16.337253100Z"
    }
   },
   "id": "d94683d48cedce60",
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "class BCModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BCModel, self).__init__()\n",
    "        self.fc1 = nn.Linear(2, 32)\n",
    "        self.dropout1 = nn.Dropout(0.5)  # Dropout with a probability of 0.5\n",
    "        self.fc2 = nn.Linear(32, 32)\n",
    "        self.dropout2 = nn.Dropout(0.5)  # Dropout with a probability of 0.5\n",
    "        self.fc3 = nn.Linear(32, 4)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.dropout1(x)\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.dropout2(x)\n",
    "        x = self.fc3(x)\n",
    "        return torch.softmax(x, dim=1)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-14T00:33:16.383213300Z",
     "start_time": "2024-06-14T00:33:16.370356600Z"
    }
   },
   "id": "41d15b78e9b8de77",
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for Subject 0\n",
      "Subject 0 - Train Custom Loss: 0.9808379570643109\n",
      "Subject 0 - Train Dataset Average Deviation: 0.6222222222222222\n",
      "Subject 0 - Test Custom Loss: 0.9831166055467394\n",
      "Subject 0 - Test Set Average Deviation: 0.6222222222222222\n",
      "Training for Subject 1\n",
      "Subject 1 - Train Custom Loss: 1.1951705879635282\n",
      "Subject 1 - Train Dataset Average Deviation: 0.6666666666666666\n",
      "Subject 1 - Test Custom Loss: 1.2377259148491753\n",
      "Subject 1 - Test Set Average Deviation: 0.6888888888888889\n",
      "Training for Subject 2\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[7], line 48\u001B[0m\n\u001B[0;32m     46\u001B[0m outputs \u001B[38;5;241m=\u001B[39m model(inputs)\n\u001B[0;32m     47\u001B[0m loss \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m-\u001B[39mtorch\u001B[38;5;241m.\u001B[39msum(torch\u001B[38;5;241m.\u001B[39mlog(outputs\u001B[38;5;241m.\u001B[39mgather(\u001B[38;5;241m1\u001B[39m, labels\u001B[38;5;241m.\u001B[39munsqueeze(\u001B[38;5;241m1\u001B[39m))\u001B[38;5;241m.\u001B[39msqueeze(\u001B[38;5;241m1\u001B[39m)))\n\u001B[1;32m---> 48\u001B[0m \u001B[43mloss\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     49\u001B[0m optimizer\u001B[38;5;241m.\u001B[39mstep()\n\u001B[0;32m     50\u001B[0m running_loss \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m loss\u001B[38;5;241m.\u001B[39mitem()\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\CASLabv2\\lib\\site-packages\\torch\\_tensor.py:522\u001B[0m, in \u001B[0;36mTensor.backward\u001B[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001B[0m\n\u001B[0;32m    512\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m has_torch_function_unary(\u001B[38;5;28mself\u001B[39m):\n\u001B[0;32m    513\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m handle_torch_function(\n\u001B[0;32m    514\u001B[0m         Tensor\u001B[38;5;241m.\u001B[39mbackward,\n\u001B[0;32m    515\u001B[0m         (\u001B[38;5;28mself\u001B[39m,),\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    520\u001B[0m         inputs\u001B[38;5;241m=\u001B[39minputs,\n\u001B[0;32m    521\u001B[0m     )\n\u001B[1;32m--> 522\u001B[0m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mautograd\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    523\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgradient\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minputs\u001B[49m\n\u001B[0;32m    524\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\CASLabv2\\lib\\site-packages\\torch\\autograd\\__init__.py:266\u001B[0m, in \u001B[0;36mbackward\u001B[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001B[0m\n\u001B[0;32m    261\u001B[0m     retain_graph \u001B[38;5;241m=\u001B[39m create_graph\n\u001B[0;32m    263\u001B[0m \u001B[38;5;66;03m# The reason we repeat the same comment below is that\u001B[39;00m\n\u001B[0;32m    264\u001B[0m \u001B[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001B[39;00m\n\u001B[0;32m    265\u001B[0m \u001B[38;5;66;03m# calls in the traceback and some print out the last line\u001B[39;00m\n\u001B[1;32m--> 266\u001B[0m \u001B[43mVariable\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_execution_engine\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrun_backward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001B[39;49;00m\n\u001B[0;32m    267\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtensors\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    268\u001B[0m \u001B[43m    \u001B[49m\u001B[43mgrad_tensors_\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    269\u001B[0m \u001B[43m    \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    270\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    271\u001B[0m \u001B[43m    \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    272\u001B[0m \u001B[43m    \u001B[49m\u001B[43mallow_unreachable\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[0;32m    273\u001B[0m \u001B[43m    \u001B[49m\u001B[43maccumulate_grad\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[0;32m    274\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "model = BCModel()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-5)\n",
    "\n",
    "all_subjects_avg_deviation = []\n",
    "all_subjects_last_epoch_loss = []\n",
    "all_subjects_test_deviation = []\n",
    "all_subjects_custom_loss = []\n",
    "\n",
    "\n",
    "train_len = 36\n",
    "test_len = 9\n",
    "\n",
    "for subject, data_splits in cross_val_data.items():\n",
    "    print(f\"Training for Subject {subject}\")\n",
    "    subject_outputs = []\n",
    "    subject_avg_deviation = []\n",
    "    last_epoch_losses = []\n",
    "    test_deviation = []\n",
    "    custom_losses = []\n",
    "    for fold, ((train_states, train_actions), (test_states, test_actions)) in enumerate(data_splits):\n",
    "        # Data preparation includes the first state treatment\n",
    "        train_states_expanded = np.column_stack((train_states, np.roll(train_states, 1)))\n",
    "        train_states_expanded[0, 1] = train_states_expanded[0, 0]\n",
    "        \n",
    "        test_states_expanded = np.column_stack((test_states, np.roll(test_states, 1)))\n",
    "        test_states_expanded[0, 1] = test_states_expanded[0, 0]\n",
    "\n",
    "        # Convert to tensors and create dataloaders\n",
    "        train_states_tensor = torch.tensor(train_states_expanded, dtype=torch.float32)\n",
    "        train_actions_tensor = torch.tensor(train_actions, dtype=torch.long)\n",
    "        test_states_tensor = torch.tensor(test_states_expanded, dtype=torch.float32)\n",
    "        test_actions_tensor = torch.tensor(test_actions, dtype=torch.long)\n",
    "\n",
    "        train_dataset = TensorDataset(train_states_tensor, train_actions_tensor)\n",
    "        train_loader = DataLoader(train_dataset, batch_size=10, shuffle=True)\n",
    "        test_dataset = TensorDataset(test_states_tensor, test_actions_tensor)\n",
    "        test_loader = DataLoader(test_dataset, batch_size=10, shuffle=False)\n",
    "\n",
    "        # Training loop\n",
    "        model.train()\n",
    "        \n",
    "        for epoch in range(50):\n",
    "            running_loss = 0.0\n",
    "            for inputs, labels in train_loader:\n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(inputs)\n",
    "                loss = -torch.sum(torch.log(outputs.gather(1, labels.unsqueeze(1)).squeeze(1)))\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                running_loss += loss.item()\n",
    "            if epoch == 49:  # Store the last epoch loss\n",
    "                last_epoch_losses.append(running_loss / train_len)\n",
    "        \n",
    "        # Evaluate on test set and compute custom loss\n",
    "        model.eval()\n",
    "        test_total_deviation = 0.0\n",
    "        test_num_samples = 0\n",
    "        fold_custom_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in test_loader:\n",
    "                outputs = model(inputs)\n",
    "                _, predicted_classes = torch.max(outputs, 1)\n",
    "                deviation = torch.abs(predicted_classes.float() - labels.float())\n",
    "                test_total_deviation += deviation.sum().item()\n",
    "                test_num_samples += labels.size(0)\n",
    "                \n",
    "                # Compute the custom loss for selected probabilities\n",
    "                selected_probabilities = outputs.gather(1, labels.unsqueeze(1)).squeeze(1)\n",
    "                custom_loss = -torch.sum(torch.log(selected_probabilities))\n",
    "                fold_custom_loss += custom_loss.item()\n",
    "\n",
    "        fold_test_deviation = test_total_deviation / test_num_samples\n",
    "        test_deviation.append(fold_test_deviation)\n",
    "        custom_losses.append(fold_custom_loss / test_num_samples)\n",
    "\n",
    "        # Evaluate on the whole dataset after training\n",
    "        with torch.no_grad():\n",
    "            full_dataset_outputs = model(train_states_tensor)\n",
    "            _, full_dataset_predicted = torch.max(full_dataset_outputs, 1)\n",
    "            deviation = torch.abs(full_dataset_predicted.float() - train_actions_tensor.float())\n",
    "            total_deviation = deviation.sum().item()\n",
    "            num_samples = train_actions_tensor.size(0)\n",
    "            avg_deviation = total_deviation / num_samples\n",
    "            subject_avg_deviation.append(avg_deviation)\n",
    "            subject_outputs.append(full_dataset_outputs.numpy())  # Store outputs for other uses\n",
    "    \n",
    "    # After all folds for a subject\n",
    "    average_of_deviation = np.mean(subject_avg_deviation)\n",
    "    average_test_deviation = np.mean(test_deviation)\n",
    "    all_subjects_avg_deviation.append(average_of_deviation)\n",
    "    all_subjects_test_deviation.append(average_test_deviation)\n",
    "    average_last_epoch_loss = np.mean(last_epoch_losses)\n",
    "    all_subjects_last_epoch_loss.append(average_last_epoch_loss)\n",
    "    average_custom_loss = np.mean(custom_losses)\n",
    "    all_subjects_custom_loss.append(average_custom_loss)\n",
    "    \n",
    "    print(f\"Subject {subject} - Train Custom Loss: {np.mean(last_epoch_losses)}\")\n",
    "    print(f\"Subject {subject} - Train Dataset Average Deviation: {average_of_deviation}\")\n",
    "    print(f\"Subject {subject} - Test Custom Loss: {average_custom_loss}\")\n",
    "    print(f\"Subject {subject} - Test Set Average Deviation: {average_test_deviation}\")\n",
    "\n",
    "# After all subjects\n",
    "print(f\"All Subjects - Overall Average Deviation: {np.mean(all_subjects_avg_deviation)}\")\n",
    "print(f\"All Subjects - Overall Test Set Average Deviation: {np.mean(all_subjects_test_deviation)}\")\n",
    "print(f\"All Subjects - Overall Average Last Epoch Loss: {np.mean(all_subjects_last_epoch_loss)}\")\n",
    "print(f\"All Subjects - Overall Custom Loss: {np.mean(all_subjects_custom_loss)}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-10T11:01:52.398790600Z",
     "start_time": "2024-06-10T11:01:45.300008800Z"
    }
   },
   "id": "f391625ca343f252",
   "execution_count": 7
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "18f2117df2b24892"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subject 0 - Average Deviation: 0.7333333333333333\n",
      "Subject 0 - Evaluation Loss: 1.0209438747829862\n",
      "Subject 1 - Average Deviation: 0.6888888888888889\n",
      "Subject 1 - Evaluation Loss: 1.2068284776475695\n",
      "Subject 2 - Average Deviation: 0.5777777777777777\n",
      "Subject 2 - Evaluation Loss: 1.0195906533135308\n",
      "Subject 3 - Average Deviation: 0.7777777777777778\n",
      "Subject 3 - Evaluation Loss: 1.0636436992221407\n",
      "Subject 4 - Average Deviation: 0.6666666666666666\n",
      "Subject 4 - Evaluation Loss: 1.1014594184027777\n",
      "Subject 5 - Average Deviation: 0.5111111111111111\n",
      "Subject 5 - Evaluation Loss: 1.014130327436659\n",
      "Subject 6 - Average Deviation: 0.7777777777777778\n",
      "Subject 6 - Evaluation Loss: 1.0318053669399685\n",
      "Subject 7 - Average Deviation: 0.6888888888888889\n",
      "Subject 7 - Evaluation Loss: 1.1226889928181967\n",
      "Subject 8 - Average Deviation: 1.0222222222222221\n",
      "Subject 8 - Evaluation Loss: 1.136333253648546\n",
      "Subject 9 - Average Deviation: 0.7555555555555555\n",
      "Subject 9 - Evaluation Loss: 0.8984461890326606\n",
      "Subject 10 - Average Deviation: 0.28888888888888886\n",
      "Subject 10 - Evaluation Loss: 0.7340751118130154\n",
      "Subject 11 - Average Deviation: 0.8222222222222222\n",
      "Subject 11 - Evaluation Loss: 1.0577703687879774\n",
      "Subject 12 - Average Deviation: 0.6\n",
      "Subject 12 - Evaluation Loss: 1.004703532324897\n",
      "Subject 13 - Average Deviation: 0.5555555555555556\n",
      "Subject 13 - Evaluation Loss: 1.0450777265760633\n",
      "Subject 14 - Average Deviation: 0.6666666666666666\n",
      "Subject 14 - Evaluation Loss: 0.9791443824768067\n",
      "Subject 15 - Average Deviation: 0.6\n",
      "Subject 15 - Evaluation Loss: 1.066582171122233\n",
      "Subject 16 - Average Deviation: 0.6444444444444445\n",
      "Subject 16 - Evaluation Loss: 1.09371059205797\n",
      "Subject 17 - Average Deviation: 0.5777777777777777\n",
      "Subject 17 - Evaluation Loss: 1.0939477178785535\n",
      "Subject 18 - Average Deviation: 0.37777777777777777\n",
      "Subject 18 - Evaluation Loss: 0.8003024948967827\n",
      "Subject 19 - Average Deviation: 0.5111111111111111\n",
      "Subject 19 - Evaluation Loss: 0.9748679796854655\n",
      "Subject 20 - Average Deviation: 0.7555555555555555\n",
      "Subject 20 - Evaluation Loss: 1.0714613914489746\n",
      "Subject 21 - Average Deviation: 0.4888888888888889\n",
      "Subject 21 - Evaluation Loss: 0.9929065810309516\n",
      "Subject 22 - Average Deviation: 0.6666666666666666\n",
      "Subject 22 - Evaluation Loss: 1.1845105489095051\n",
      "Subject 23 - Average Deviation: 0.7777777777777778\n",
      "Subject 23 - Evaluation Loss: 1.1599210951063368\n",
      "Subject 24 - Average Deviation: 0.9555555555555556\n",
      "Subject 24 - Evaluation Loss: 1.2080403963724773\n",
      "Subject 25 - Average Deviation: 0.6666666666666666\n",
      "Subject 25 - Evaluation Loss: 1.0828063011169433\n",
      "Subject 26 - Average Deviation: 0.9333333333333333\n",
      "Subject 26 - Evaluation Loss: 1.2946830325656467\n",
      "Subject 27 - Average Deviation: 0.6\n",
      "Subject 27 - Evaluation Loss: 1.0666614532470704\n",
      "Subject 28 - Average Deviation: 0.7333333333333333\n",
      "Subject 28 - Evaluation Loss: 1.065567101372613\n",
      "Subject 29 - Average Deviation: 0.5111111111111111\n",
      "Subject 29 - Evaluation Loss: 0.9597510072920058\n",
      "Subject 30 - Average Deviation: 0.6222222222222222\n",
      "Subject 30 - Evaluation Loss: 1.113091935051812\n",
      "Subject 31 - Average Deviation: 0.6444444444444445\n",
      "Subject 31 - Evaluation Loss: 1.0877052519056531\n",
      "Subject 32 - Average Deviation: 0.5555555555555556\n",
      "Subject 32 - Evaluation Loss: 1.0689453125\n",
      "Subject 33 - Average Deviation: 0.5333333333333333\n",
      "Subject 33 - Evaluation Loss: 0.9419375313652887\n",
      "Subject 34 - Average Deviation: 0.5111111111111111\n",
      "Subject 34 - Evaluation Loss: 0.9563890086279975\n",
      "Subject 35 - Average Deviation: 0.5555555555555556\n",
      "Subject 35 - Evaluation Loss: 1.14431488249037\n",
      "Subject 36 - Average Deviation: 0.6666666666666666\n",
      "Subject 36 - Evaluation Loss: 1.1445388899909126\n",
      "Subject 37 - Average Deviation: 0.6444444444444445\n",
      "Subject 37 - Evaluation Loss: 1.0805941687689886\n",
      "Subject 38 - Average Deviation: 0.7555555555555555\n",
      "Subject 38 - Evaluation Loss: 1.219191222720676\n",
      "Subject 39 - Average Deviation: 0.8222222222222222\n",
      "Subject 39 - Evaluation Loss: 1.120425213707818\n",
      "Subject 40 - Average Deviation: 0.6222222222222222\n",
      "Subject 40 - Evaluation Loss: 0.9664500448438856\n",
      "Subject 41 - Average Deviation: 0.8444444444444444\n",
      "Subject 41 - Evaluation Loss: 1.2218584484524198\n",
      "Subject 42 - Average Deviation: 0.7333333333333333\n",
      "Subject 42 - Evaluation Loss: 1.123011843363444\n",
      "Subject 43 - Average Deviation: 0.9777777777777777\n",
      "Subject 43 - Evaluation Loss: 1.2461946169535318\n",
      "Subject 44 - Average Deviation: 0.8666666666666667\n",
      "Subject 44 - Evaluation Loss: 1.105434364742703\n",
      "Subject 45 - Average Deviation: 0.8444444444444444\n",
      "Subject 45 - Evaluation Loss: 1.1196532249450684\n",
      "Subject 46 - Average Deviation: 0.6444444444444445\n",
      "Subject 46 - Evaluation Loss: 1.0106113751729329\n",
      "Subject 47 - Average Deviation: 0.8666666666666667\n",
      "Subject 47 - Evaluation Loss: 1.0784185303582086\n",
      "Subject 48 - Average Deviation: 0.8222222222222222\n",
      "Subject 48 - Evaluation Loss: 1.1363476753234862\n",
      "Subject 49 - Average Deviation: 0.5555555555555556\n",
      "Subject 49 - Evaluation Loss: 0.9664895057678222\n",
      "Subject 50 - Average Deviation: 0.9555555555555556\n",
      "Subject 50 - Evaluation Loss: 1.2275350464714898\n",
      "Subject 51 - Average Deviation: 0.5111111111111111\n",
      "Subject 51 - Evaluation Loss: 0.9567627694871691\n",
      "Subject 52 - Average Deviation: 0.9555555555555556\n",
      "Subject 52 - Evaluation Loss: 1.1608078320821127\n",
      "Subject 53 - Average Deviation: 0.6444444444444445\n",
      "Subject 53 - Evaluation Loss: 1.0328404638502333\n",
      "Subject 54 - Average Deviation: 0.6888888888888889\n",
      "Subject 54 - Evaluation Loss: 1.0563175201416015\n",
      "Subject 55 - Average Deviation: 0.6\n",
      "Subject 55 - Evaluation Loss: 1.0865150663587781\n",
      "Subject 56 - Average Deviation: 0.5555555555555556\n",
      "Subject 56 - Evaluation Loss: 1.0522132025824653\n",
      "Subject 57 - Average Deviation: 0.6666666666666666\n",
      "Subject 57 - Evaluation Loss: 1.0791512383355035\n",
      "Subject 58 - Average Deviation: 0.6222222222222222\n",
      "Subject 58 - Evaluation Loss: 0.8874194039238824\n",
      "Subject 59 - Average Deviation: 0.7111111111111111\n",
      "Subject 59 - Evaluation Loss: 1.0766576237148708\n",
      "Subject 60 - Average Deviation: 0.6222222222222222\n",
      "Subject 60 - Evaluation Loss: 0.9645160992940267\n",
      "Subject 61 - Average Deviation: 0.5777777777777777\n",
      "Subject 61 - Evaluation Loss: 0.9518031120300293\n",
      "Subject 62 - Average Deviation: 0.6222222222222222\n",
      "Subject 62 - Evaluation Loss: 1.0524977684020995\n",
      "Subject 63 - Average Deviation: 0.9555555555555556\n",
      "Subject 63 - Evaluation Loss: 1.1802528699239094\n",
      "Subject 64 - Average Deviation: 0.7333333333333333\n",
      "Subject 64 - Evaluation Loss: 1.1800567838880751\n",
      "Subject 65 - Average Deviation: 0.5555555555555556\n",
      "Subject 65 - Evaluation Loss: 1.042249902089437\n",
      "Subject 66 - Average Deviation: 0.7333333333333333\n",
      "Subject 66 - Evaluation Loss: 1.1237175199720595\n",
      "Subject 67 - Average Deviation: 0.7555555555555555\n",
      "Subject 67 - Evaluation Loss: 1.0866361300150553\n",
      "Subject 68 - Average Deviation: 0.7111111111111111\n",
      "Subject 68 - Evaluation Loss: 1.1675819291008844\n",
      "Subject 69 - Average Deviation: 0.5777777777777777\n",
      "Subject 69 - Evaluation Loss: 1.0134463204277886\n",
      "Subject 70 - Average Deviation: 0.6\n",
      "Subject 70 - Evaluation Loss: 1.0948884116278754\n",
      "Subject 71 - Average Deviation: 0.6444444444444445\n",
      "Subject 71 - Evaluation Loss: 1.0386069933573405\n",
      "Subject 72 - Average Deviation: 0.5555555555555556\n",
      "Subject 72 - Evaluation Loss: 0.9863322840796577\n",
      "Subject 73 - Average Deviation: 0.7111111111111111\n",
      "Subject 73 - Evaluation Loss: 0.9201808717515734\n",
      "Subject 74 - Average Deviation: 0.6444444444444445\n",
      "Subject 74 - Evaluation Loss: 1.0189112769232855\n",
      "Subject 75 - Average Deviation: 0.6666666666666666\n",
      "Subject 75 - Evaluation Loss: 1.189609506395128\n",
      "Subject 76 - Average Deviation: 0.6888888888888889\n",
      "Subject 76 - Evaluation Loss: 1.0661538336012097\n",
      "Subject 77 - Average Deviation: 0.5555555555555556\n",
      "Subject 77 - Evaluation Loss: 1.0666334576076932\n",
      "Subject 78 - Average Deviation: 0.6888888888888889\n",
      "Subject 78 - Evaluation Loss: 1.0087545394897461\n",
      "Subject 79 - Average Deviation: 0.8666666666666667\n",
      "Subject 79 - Evaluation Loss: 1.2889938460456\n",
      "Subject 80 - Average Deviation: 0.6444444444444445\n",
      "Subject 80 - Evaluation Loss: 1.0218459129333497\n",
      "Subject 81 - Average Deviation: 0.4888888888888889\n",
      "Subject 81 - Evaluation Loss: 0.9362160311804877\n",
      "Subject 82 - Average Deviation: 0.4888888888888889\n",
      "Subject 82 - Evaluation Loss: 0.884973578982883\n",
      "Subject 83 - Average Deviation: 0.5555555555555556\n",
      "Subject 83 - Evaluation Loss: 1.0214452107747396\n",
      "Subject 84 - Average Deviation: 0.4444444444444444\n",
      "Subject 84 - Evaluation Loss: 0.9412978066338433\n",
      "Subject 85 - Average Deviation: 0.6222222222222222\n",
      "Subject 85 - Evaluation Loss: 1.0075930913289388\n",
      "Subject 86 - Average Deviation: 0.8222222222222222\n",
      "Subject 86 - Evaluation Loss: 1.1388573540581597\n",
      "Subject 87 - Average Deviation: 0.9555555555555556\n",
      "Subject 87 - Evaluation Loss: 1.205580563015408\n",
      "Subject 88 - Average Deviation: 1.0\n",
      "Subject 88 - Evaluation Loss: 1.134861977895101\n",
      "Subject 89 - Average Deviation: 0.6444444444444445\n",
      "Subject 89 - Evaluation Loss: 1.1752115037706163\n",
      "Subject 90 - Average Deviation: 0.9111111111111111\n",
      "Subject 90 - Evaluation Loss: 1.213885858323839\n",
      "Subject 91 - Average Deviation: 0.9333333333333333\n",
      "Subject 91 - Evaluation Loss: 1.1754514588250053\n",
      "Subject 92 - Average Deviation: 0.6444444444444445\n",
      "Subject 92 - Evaluation Loss: 1.0951692793104384\n",
      "Subject 93 - Average Deviation: 0.7555555555555555\n",
      "Subject 93 - Evaluation Loss: 1.179870817396376\n",
      "Subject 94 - Average Deviation: 0.6222222222222222\n",
      "Subject 94 - Evaluation Loss: 1.075922711690267\n",
      "Subject 95 - Average Deviation: 0.6666666666666666\n",
      "Subject 95 - Evaluation Loss: 0.9949173821343316\n",
      "Subject 96 - Average Deviation: 0.8\n",
      "Subject 96 - Evaluation Loss: 1.0793767293294272\n",
      "Subject 97 - Average Deviation: 0.4\n",
      "Subject 97 - Evaluation Loss: 0.9101411289638943\n",
      "Subject 98 - Average Deviation: 1.0444444444444445\n",
      "Subject 98 - Evaluation Loss: 1.269994682735867\n",
      "Subject 99 - Average Deviation: 0.6222222222222222\n",
      "Subject 99 - Evaluation Loss: 1.0787529627482095\n",
      "Subject 100 - Average Deviation: 0.8222222222222222\n",
      "Subject 100 - Evaluation Loss: 1.0643997033437094\n",
      "Subject 101 - Average Deviation: 0.6444444444444445\n",
      "Subject 101 - Evaluation Loss: 1.02971158557468\n",
      "Subject 102 - Average Deviation: 0.8444444444444444\n",
      "Subject 102 - Evaluation Loss: 1.138355827331543\n",
      "Subject 103 - Average Deviation: 0.6888888888888889\n",
      "Subject 103 - Evaluation Loss: 1.1924683782789443\n",
      "Subject 104 - Average Deviation: 0.4888888888888889\n",
      "Subject 104 - Evaluation Loss: 0.9648142284817166\n",
      "Subject 105 - Average Deviation: 0.5111111111111111\n",
      "Subject 105 - Evaluation Loss: 1.0026205062866211\n",
      "Subject 106 - Average Deviation: 0.9111111111111111\n",
      "Subject 106 - Evaluation Loss: 1.1397991074456109\n",
      "Subject 107 - Average Deviation: 0.7333333333333333\n",
      "Subject 107 - Evaluation Loss: 1.0192842271592881\n",
      "Subject 108 - Average Deviation: 0.5333333333333333\n",
      "Subject 108 - Evaluation Loss: 0.9308071136474609\n",
      "Subject 109 - Average Deviation: 0.6666666666666666\n",
      "Subject 109 - Evaluation Loss: 1.1091122998131646\n",
      "Subject 110 - Average Deviation: 0.5333333333333333\n",
      "Subject 110 - Evaluation Loss: 0.9256629519992404\n",
      "Subject 111 - Average Deviation: 0.5111111111111111\n",
      "Subject 111 - Evaluation Loss: 0.838524415757921\n",
      "Subject 112 - Average Deviation: 0.7333333333333333\n",
      "Subject 112 - Evaluation Loss: 1.1390346527099608\n",
      "Subject 113 - Average Deviation: 0.4888888888888889\n",
      "Subject 113 - Evaluation Loss: 0.984427113003201\n",
      "Subject 114 - Average Deviation: 0.4444444444444444\n",
      "Subject 114 - Evaluation Loss: 0.921368334028456\n",
      "Subject 115 - Average Deviation: 0.6444444444444445\n",
      "Subject 115 - Evaluation Loss: 1.0230177031622993\n",
      "Subject 116 - Average Deviation: 0.8222222222222222\n",
      "Subject 116 - Evaluation Loss: 1.1085031615363228\n",
      "Subject 117 - Average Deviation: 0.7111111111111111\n",
      "Subject 117 - Evaluation Loss: 1.1600090980529785\n",
      "Subject 118 - Average Deviation: 0.7777777777777778\n",
      "Subject 118 - Evaluation Loss: 1.0949192576938205\n",
      "Subject 119 - Average Deviation: 0.5333333333333333\n",
      "Subject 119 - Evaluation Loss: 0.9451594034830729\n",
      "Subject 120 - Average Deviation: 0.7555555555555555\n",
      "Subject 120 - Evaluation Loss: 1.116801208919949\n",
      "Subject 121 - Average Deviation: 0.8444444444444444\n",
      "Subject 121 - Evaluation Loss: 1.1434534284803601\n",
      "Subject 122 - Average Deviation: 0.6222222222222222\n",
      "Subject 122 - Evaluation Loss: 0.9846721755133735\n",
      "Subject 123 - Average Deviation: 0.5555555555555556\n",
      "Subject 123 - Evaluation Loss: 1.101689550611708\n",
      "Subject 124 - Average Deviation: 0.6222222222222222\n",
      "Subject 124 - Evaluation Loss: 0.9385829713609484\n",
      "Subject 125 - Average Deviation: 0.7111111111111111\n",
      "Subject 125 - Evaluation Loss: 1.2534301333957247\n",
      "Subject 126 - Average Deviation: 0.6666666666666666\n",
      "Subject 126 - Evaluation Loss: 1.1292581346299913\n",
      "Subject 127 - Average Deviation: 0.5555555555555556\n",
      "Subject 127 - Evaluation Loss: 1.0069923718770346\n",
      "Subject 128 - Average Deviation: 0.5111111111111111\n",
      "Subject 128 - Evaluation Loss: 0.9836005846659343\n",
      "Subject 129 - Average Deviation: 0.6444444444444445\n",
      "Subject 129 - Evaluation Loss: 1.0143459531995984\n",
      "Subject 130 - Average Deviation: 0.7777777777777778\n",
      "Subject 130 - Evaluation Loss: 1.1470442771911622\n",
      "Subject 131 - Average Deviation: 0.9777777777777777\n",
      "Subject 131 - Evaluation Loss: 1.1206663873460558\n",
      "Subject 132 - Average Deviation: 0.7333333333333333\n",
      "Subject 132 - Evaluation Loss: 1.0859341515435113\n",
      "Subject 133 - Average Deviation: 0.4888888888888889\n",
      "Subject 133 - Evaluation Loss: 0.9812613169352213\n",
      "Subject 134 - Average Deviation: 0.5555555555555556\n",
      "Subject 134 - Evaluation Loss: 0.9069408416748047\n",
      "Subject 135 - Average Deviation: 0.6888888888888889\n",
      "Subject 135 - Evaluation Loss: 1.0488511933220757\n",
      "Subject 136 - Average Deviation: 0.5777777777777777\n",
      "Subject 136 - Evaluation Loss: 1.0142452875773111\n",
      "Subject 137 - Average Deviation: 0.7777777777777778\n",
      "Subject 137 - Evaluation Loss: 1.125379000769721\n",
      "Subject 138 - Average Deviation: 0.8\n",
      "Subject 138 - Evaluation Loss: 1.0668735398186577\n",
      "Subject 139 - Average Deviation: 0.7333333333333333\n",
      "Subject 139 - Evaluation Loss: 1.0613228268093533\n",
      "Subject 140 - Average Deviation: 0.6666666666666666\n",
      "Subject 140 - Evaluation Loss: 1.11960572136773\n",
      "Subject 141 - Average Deviation: 0.5333333333333333\n",
      "Subject 141 - Evaluation Loss: 0.9293439653184679\n",
      "Subject 142 - Average Deviation: 0.9555555555555556\n",
      "Subject 142 - Evaluation Loss: 1.1447569688161214\n",
      "Subject 143 - Average Deviation: 0.6222222222222222\n",
      "Subject 143 - Evaluation Loss: 1.11193175845676\n",
      "Subject 144 - Average Deviation: 0.6666666666666666\n",
      "Subject 144 - Evaluation Loss: 1.014912986755371\n",
      "Subject 145 - Average Deviation: 0.5555555555555556\n",
      "Subject 145 - Evaluation Loss: 1.0648655573527017\n",
      "Subject 146 - Average Deviation: 0.5777777777777777\n",
      "Subject 146 - Evaluation Loss: 1.0580722914801703\n",
      "Subject 147 - Average Deviation: 0.7555555555555555\n",
      "Subject 147 - Evaluation Loss: 1.088258859846327\n",
      "Subject 148 - Average Deviation: 0.7111111111111111\n",
      "Subject 148 - Evaluation Loss: 1.0636684947543673\n",
      "Subject 149 - Average Deviation: 0.5333333333333333\n",
      "Subject 149 - Evaluation Loss: 0.9403724246554904\n",
      "Model outputs for specific inputs across all subjects:\n",
      "Subject 0 Outputs:\n",
      "[[0.28795505 0.16379717 0.20148192 0.3467658 ]\n",
      " [0.47700673 0.11373572 0.17789644 0.23136109]\n",
      " [0.11525543 0.14385    0.1609354  0.57995915]\n",
      " [0.12320498 0.11575945 0.16230999 0.5987256 ]]\n",
      "Subject 1 Outputs:\n",
      "[[0.31164536 0.32230547 0.21617943 0.14986981]\n",
      " [0.36244816 0.3256212  0.19478193 0.11714873]\n",
      " [0.07984774 0.24552882 0.37149543 0.303128  ]\n",
      " [0.09699935 0.26569858 0.36706525 0.27023688]]\n",
      "Subject 2 Outputs:\n",
      "[[0.67985535 0.176637   0.09162865 0.05187899]\n",
      " [0.7725288  0.13232285 0.0689183  0.02622996]\n",
      " [0.16021807 0.20068312 0.31099933 0.32809952]\n",
      " [0.10243398 0.17131346 0.37347448 0.35277808]]\n",
      "Subject 3 Outputs:\n",
      "[[0.6007747  0.20085366 0.05791166 0.14046003]\n",
      " [0.70639646 0.1565903  0.04388369 0.09312954]\n",
      " [0.11600015 0.18026294 0.27221265 0.4315243 ]\n",
      " [0.06054485 0.13946974 0.38721573 0.4127697 ]]\n",
      "Subject 4 Outputs:\n",
      "[[0.59375733 0.22117585 0.09831078 0.08675607]\n",
      " [0.79278666 0.12772492 0.04974888 0.02973957]\n",
      " [0.10180057 0.23983271 0.29336026 0.36500648]\n",
      " [0.16674727 0.24531314 0.29715207 0.29078752]]\n",
      "Subject 5 Outputs:\n",
      "[[0.52139723 0.34720445 0.08225492 0.04914349]\n",
      " [0.6271973  0.323277   0.035636   0.01388964]\n",
      " [0.04179654 0.17219312 0.47690287 0.30910745]\n",
      " [0.06828501 0.20952497 0.4441856  0.27800444]]\n",
      "Subject 6 Outputs:\n",
      "[[0.7931063  0.14985299 0.04072284 0.01631779]\n",
      " [0.8377542  0.12366548 0.03017906 0.00840126]\n",
      " [0.18739751 0.23193076 0.25326344 0.3274083 ]\n",
      " [0.1378889  0.22062059 0.29822785 0.34326267]]\n",
      "Subject 7 Outputs:\n",
      "[[0.41252026 0.26509234 0.28446046 0.03792689]\n",
      " [0.40885505 0.26359147 0.29948375 0.02806978]\n",
      " [0.07402349 0.14563747 0.49936262 0.28097638]\n",
      " [0.08029099 0.15146522 0.5066863  0.26155752]]\n",
      "Subject 8 Outputs:\n",
      "[[0.6224164  0.25562087 0.08566944 0.03629327]\n",
      " [0.6645304  0.26544395 0.05591946 0.01410613]\n",
      " [0.28500286 0.18477184 0.23621695 0.29400837]\n",
      " [0.28396067 0.18627137 0.24353677 0.28623122]]\n",
      "Subject 9 Outputs:\n",
      "[[8.7480915e-01 9.7227059e-02 2.2292329e-02 5.6714397e-03]\n",
      " [9.5428228e-01 4.1311745e-02 3.9702365e-03 4.3577846e-04]\n",
      " [2.7617747e-01 1.8585490e-01 2.6110828e-01 2.7685934e-01]\n",
      " [4.0968645e-01 2.1882027e-01 2.0166342e-01 1.6982977e-01]]\n",
      "Subject 10 Outputs:\n",
      "[[8.13228726e-01 1.07397825e-01 7.13433698e-02 8.03012215e-03]\n",
      " [9.30953920e-01 4.42684926e-02 2.39379574e-02 8.39715067e-04]\n",
      " [3.81897986e-02 1.02034949e-01 5.83916485e-01 2.75858760e-01]\n",
      " [5.99636622e-02 1.28098607e-01 5.92814028e-01 2.19123766e-01]]\n",
      "Subject 11 Outputs:\n",
      "[[0.7580745  0.18169345 0.05322316 0.0070089 ]\n",
      " [0.7384795  0.20128152 0.05639985 0.00383912]\n",
      " [0.162664   0.22494167 0.22905616 0.38333818]\n",
      " [0.16555177 0.23864959 0.27116027 0.3246384 ]]\n",
      "Subject 12 Outputs:\n",
      "[[0.54582185 0.33790612 0.07886738 0.03740467]\n",
      " [0.50274944 0.3444035  0.11223993 0.04060705]\n",
      " [0.03921986 0.10383141 0.26236662 0.5945821 ]\n",
      " [0.05878149 0.12808692 0.3429623  0.4701694 ]]\n",
      "Subject 13 Outputs:\n",
      "[[0.6544176  0.2833072  0.03266237 0.02961291]\n",
      " [0.6463641  0.3059067  0.03071209 0.01701718]\n",
      " [0.05488825 0.27400225 0.37390792 0.29720154]\n",
      " [0.078199   0.27748105 0.4037335  0.2405864 ]]\n",
      "Subject 14 Outputs:\n",
      "[[0.62279963 0.14398175 0.0647074  0.16851118]\n",
      " [0.5665718  0.1801385  0.08394739 0.16934228]\n",
      " [0.0132321  0.11370111 0.37728065 0.49578613]\n",
      " [0.02230577 0.14364491 0.40800595 0.42604336]]\n",
      "Subject 15 Outputs:\n",
      "[[0.7720971  0.11722716 0.04340647 0.06726918]\n",
      " [0.693866   0.1707875  0.06540292 0.06994355]\n",
      " [0.07483836 0.29814154 0.37058762 0.25643247]\n",
      " [0.13463545 0.28138983 0.34132054 0.24265417]]\n",
      "Subject 16 Outputs:\n",
      "[[0.5707684  0.18868199 0.12617758 0.11437208]\n",
      " [0.8023199  0.12514406 0.04409548 0.0284406 ]\n",
      " [0.07044285 0.2730686  0.37914044 0.2773481 ]\n",
      " [0.10900873 0.26888943 0.358703   0.26339895]]\n",
      "Subject 17 Outputs:\n",
      "[[0.50947016 0.21564785 0.13161404 0.14326793]\n",
      " [0.6014181  0.22746505 0.09124369 0.07987325]\n",
      " [0.04385924 0.18402834 0.47929892 0.2928135 ]\n",
      " [0.04937905 0.18612473 0.48099178 0.28350446]]\n",
      "Subject 18 Outputs:\n",
      "[[0.79851866 0.14321646 0.02845674 0.0298082 ]\n",
      " [0.9057146  0.08366937 0.00576865 0.00484739]\n",
      " [0.01881462 0.23330598 0.44718298 0.30069637]\n",
      " [0.02630356 0.24919225 0.44601333 0.27849084]]\n",
      "Subject 19 Outputs:\n",
      "[[0.6012347  0.24925151 0.11155307 0.03796076]\n",
      " [0.6842396  0.23492993 0.06903856 0.0117918 ]\n",
      " [0.00749388 0.18265471 0.4279812  0.38187015]\n",
      " [0.01021658 0.20488386 0.4435706  0.34132895]]\n",
      "Subject 20 Outputs:\n",
      "[[0.6913302  0.22433496 0.03968136 0.04465347]\n",
      " [0.7269882  0.21512292 0.02884931 0.02903959]\n",
      " [0.0831393  0.22293594 0.2439126  0.4500121 ]\n",
      " [0.10598344 0.24375841 0.27252135 0.37773684]]\n",
      "Subject 21 Outputs:\n",
      "[[0.67709064 0.23797922 0.06230165 0.02262851]\n",
      " [0.788888   0.16699535 0.0356384  0.00847831]\n",
      " [0.04812847 0.21300264 0.41717365 0.3216952 ]\n",
      " [0.10149469 0.24166785 0.42140147 0.23543598]]\n",
      "Subject 22 Outputs:\n",
      "[[0.5310509  0.2973366  0.08635452 0.08525801]\n",
      " [0.52377075 0.2962316  0.09731769 0.08267996]\n",
      " [0.07327949 0.25394127 0.40695223 0.26582706]\n",
      " [0.08903452 0.2521529  0.42795444 0.23085815]]\n",
      "Subject 23 Outputs:\n",
      "[[0.5910166  0.29108936 0.02897153 0.08892254]\n",
      " [0.56685686 0.28993493 0.0400877  0.10312055]\n",
      " [0.098805   0.2967097  0.28722623 0.31725904]\n",
      " [0.23020805 0.2878151  0.2508957  0.23108117]]\n",
      "Subject 24 Outputs:\n",
      "[[0.59886867 0.25321728 0.06919195 0.07872209]\n",
      " [0.570855   0.23987852 0.08746111 0.10180531]\n",
      " [0.06930713 0.24453679 0.3479052  0.33825088]\n",
      " [0.27974233 0.23783858 0.2548991  0.22752003]]\n",
      "Subject 25 Outputs:\n",
      "[[0.45214644 0.21906921 0.21838489 0.11039944]\n",
      " [0.45635957 0.20353264 0.23030563 0.10980224]\n",
      " [0.00568964 0.12023607 0.48060128 0.39347303]\n",
      " [0.02947943 0.16548309 0.4664993  0.33853814]]\n",
      "Subject 26 Outputs:\n",
      "[[0.43632403 0.28530192 0.19884656 0.07952743]\n",
      " [0.44292015 0.29257038 0.20223227 0.0622772 ]\n",
      " [0.14169492 0.28620404 0.28816336 0.2839377 ]\n",
      " [0.21807952 0.28771812 0.29397982 0.20022257]]\n",
      "Subject 27 Outputs:\n",
      "[[0.7261301  0.17295985 0.08024176 0.0206683 ]\n",
      " [0.80709815 0.13559578 0.04902609 0.00827995]\n",
      " [0.29854593 0.24149722 0.26107457 0.19888228]\n",
      " [0.22201394 0.256904   0.303902   0.21718007]]\n",
      "Subject 28 Outputs:\n",
      "[[0.516433   0.22613506 0.2250492  0.03238266]\n",
      " [0.5271467  0.22257294 0.22949411 0.0207862 ]\n",
      " [0.03508924 0.20863305 0.33798203 0.41829568]\n",
      " [0.03391634 0.22095633 0.34386355 0.40126377]]\n",
      "Subject 29 Outputs:\n",
      "[[0.8337356  0.10359976 0.04403352 0.01863117]\n",
      " [0.7702869  0.1310083  0.06853897 0.03016573]\n",
      " [0.05780712 0.27242965 0.34664226 0.323121  ]\n",
      " [0.03587726 0.29287764 0.3581711  0.31307402]]\n",
      "Subject 30 Outputs:\n",
      "[[0.5780819  0.30755794 0.07317305 0.04118716]\n",
      " [0.537238   0.3162223  0.09365035 0.0528893 ]\n",
      " [0.063335   0.23878516 0.36790955 0.32997027]\n",
      " [0.02994946 0.25394785 0.40036762 0.31573504]]\n",
      "Subject 31 Outputs:\n",
      "[[0.52727896 0.18274511 0.18707757 0.10289837]\n",
      " [0.49554455 0.19562371 0.20045723 0.10837454]\n",
      " [0.02136632 0.19650215 0.47255808 0.30957344]\n",
      " [0.01099438 0.20093487 0.4969729  0.29109782]]\n",
      "Subject 32 Outputs:\n",
      "[[0.56285185 0.2994812  0.1113913  0.0262756 ]\n",
      " [0.56427324 0.2888905  0.11987507 0.02696115]\n",
      " [0.07870439 0.27609465 0.45645204 0.18874888]\n",
      " [0.04330696 0.29182515 0.50354636 0.16132154]]\n",
      "Subject 33 Outputs:\n",
      "[[0.5434278  0.33843952 0.10207145 0.01606124]\n",
      " [0.49735576 0.35857606 0.12394947 0.02011874]\n",
      " [0.01690718 0.13025174 0.50551367 0.34732744]\n",
      " [0.01347684 0.13277236 0.51065356 0.3430972 ]]\n",
      "Subject 34 Outputs:\n",
      "[[0.64768034 0.33081338 0.01850327 0.00300298]\n",
      " [0.6188738  0.36373657 0.01545355 0.00193617]\n",
      " [0.01895958 0.16752605 0.43983224 0.37368214]\n",
      " [0.04825506 0.23454109 0.39762053 0.3195834 ]]\n",
      "Subject 35 Outputs:\n",
      "[[0.28256214 0.42799664 0.2515646  0.03787673]\n",
      " [0.22942442 0.50157255 0.24854495 0.02045803]\n",
      " [0.13981922 0.40272322 0.31422946 0.14322811]\n",
      " [0.12688746 0.5309733  0.2797271  0.06241218]]\n",
      "Subject 36 Outputs:\n",
      "[[0.48820853 0.23686452 0.20668216 0.06824481]\n",
      " [0.52109325 0.18369377 0.22298484 0.07222804]\n",
      " [0.041363   0.253465   0.3729016  0.3322703 ]\n",
      " [0.06560296 0.2943422  0.34430823 0.29574662]]\n",
      "Subject 37 Outputs:\n",
      "[[0.61013377 0.14305568 0.15276219 0.09404833]\n",
      " [0.56235707 0.10901016 0.19506374 0.13356906]\n",
      " [0.02108127 0.23716053 0.501831   0.23992726]\n",
      " [0.05611274 0.29081726 0.41931275 0.23375721]]\n",
      "Subject 38 Outputs:\n",
      "[[0.48646915 0.29203668 0.17940088 0.04209329]\n",
      " [0.48392525 0.22909637 0.22419478 0.06278358]\n",
      " [0.14410639 0.24945205 0.43034393 0.17609766]\n",
      " [0.17878866 0.2818364  0.3843268  0.15504815]]\n",
      "Subject 39 Outputs:\n",
      "[[0.65405613 0.17852259 0.07505376 0.09236755]\n",
      " [0.6905194  0.13597286 0.07742957 0.09607825]\n",
      " [0.10594556 0.22785263 0.37067008 0.29553166]\n",
      " [0.34598207 0.22707699 0.21675977 0.21018115]]\n",
      "Subject 40 Outputs:\n",
      "[[0.5281449  0.1556013  0.12676837 0.18948539]\n",
      " [0.5132865  0.1384075  0.1446627  0.2036434 ]\n",
      " [0.00394942 0.05520783 0.25218084 0.688662  ]\n",
      " [0.01978046 0.10396949 0.28580335 0.5904467 ]]\n",
      "Subject 41 Outputs:\n",
      "[[0.52655697 0.24850193 0.11640982 0.1085313 ]\n",
      " [0.54271173 0.21698098 0.11746153 0.12284583]\n",
      " [0.0825073  0.33594808 0.26230714 0.31923753]\n",
      " [0.322271   0.28463042 0.18671086 0.20638776]]\n",
      "Subject 42 Outputs:\n",
      "[[0.6951756  0.1977271  0.07088583 0.03621146]\n",
      " [0.68391097 0.19065525 0.07816007 0.04727364]\n",
      " [0.19870958 0.27541918 0.26709297 0.25877824]\n",
      " [0.18830499 0.25974706 0.2732065  0.27874145]]\n",
      "Subject 43 Outputs:\n",
      "[[0.38774166 0.4065832  0.14065355 0.06502164]\n",
      " [0.3850003  0.37317386 0.15697949 0.08484641]\n",
      " [0.08516864 0.2803891  0.28603858 0.34840366]\n",
      " [0.10610053 0.28275132 0.27396238 0.33718574]]\n",
      "Subject 44 Outputs:\n",
      "[[0.43660712 0.36255634 0.17311563 0.02772087]\n",
      " [0.41234013 0.35110343 0.19212069 0.04443582]\n",
      " [0.03011516 0.34021667 0.2954855  0.33418265]\n",
      " [0.04158999 0.33403084 0.2660655  0.35831377]]\n",
      "Subject 45 Outputs:\n",
      "[[0.61482894 0.16762626 0.1482402  0.06930459]\n",
      " [0.5168674  0.19659606 0.18214574 0.10439077]\n",
      " [0.0201121  0.27709574 0.21614183 0.48665035]\n",
      " [0.04487789 0.2989884  0.2136294  0.44250435]]\n",
      "Subject 46 Outputs:\n",
      "[[0.51408297 0.37610295 0.08839662 0.0214175 ]\n",
      " [0.48847765 0.4127929  0.08099223 0.01773728]\n",
      " [0.00993452 0.23611878 0.39098376 0.362963  ]\n",
      " [0.02550009 0.26171482 0.34158048 0.3712046 ]]\n",
      "Subject 47 Outputs:\n",
      "[[0.4804346  0.2866709  0.17333077 0.05956373]\n",
      " [0.4922486  0.32211512 0.15205795 0.03357836]\n",
      " [0.00220482 0.18875833 0.3452477  0.4637892 ]\n",
      " [0.0177249  0.3110448  0.32724026 0.3439901 ]]\n",
      "Subject 48 Outputs:\n",
      "[[0.6545059  0.15373527 0.14230418 0.04945461]\n",
      " [0.62887114 0.16361268 0.1555455  0.05197068]\n",
      " [0.02905987 0.17869024 0.32713658 0.4651133 ]\n",
      " [0.06955867 0.26537183 0.31114334 0.3539262 ]]\n",
      "Subject 49 Outputs:\n",
      "[[0.55688775 0.2245758  0.13200557 0.08653098]\n",
      " [0.75099194 0.16687794 0.0569748  0.02515527]\n",
      " [0.01031758 0.10359165 0.48523962 0.40085122]\n",
      " [0.05361047 0.22113745 0.3397495  0.38550258]]\n",
      "Subject 50 Outputs:\n",
      "[[0.3620397  0.3841381  0.19479822 0.05902402]\n",
      " [0.34335658 0.45905116 0.17352092 0.02407146]\n",
      " [0.10649144 0.28541073 0.29675055 0.31134725]\n",
      " [0.08899764 0.32310224 0.26479402 0.3231061 ]]\n",
      "Subject 51 Outputs:\n",
      "[[0.8029858  0.1356781  0.05201279 0.00932331]\n",
      " [0.89610356 0.08270395 0.01987453 0.00131797]\n",
      " [0.1481332  0.2477131  0.33727732 0.26687637]\n",
      " [0.1633545  0.2616287  0.2913851  0.28363165]]\n",
      "Subject 52 Outputs:\n",
      "[[0.4782418  0.21666837 0.12623776 0.17885204]\n",
      " [0.5541187  0.21685088 0.10799568 0.1210348 ]\n",
      " [0.06018409 0.28217933 0.22796945 0.42966715]\n",
      " [0.04546843 0.28913882 0.21973819 0.44565448]]\n",
      "Subject 53 Outputs:\n",
      "[[0.60881746 0.17876245 0.09662829 0.11579175]\n",
      " [0.7474085  0.15310575 0.05775104 0.0417348 ]\n",
      " [0.05847624 0.1906157  0.2598287  0.49107936]\n",
      " [0.07463994 0.20620295 0.23776828 0.48138884]]\n",
      "Subject 54 Outputs:\n",
      "[[0.52931416 0.18707912 0.13464497 0.14896172]\n",
      " [0.5716631  0.19172458 0.12664954 0.10996278]\n",
      " [0.01619231 0.13516338 0.3547245  0.49391982]\n",
      " [0.02260281 0.15365547 0.34101793 0.4827238 ]]\n",
      "Subject 55 Outputs:\n",
      "[[0.6816562  0.13386747 0.14120811 0.04326821]\n",
      " [0.69553053 0.13055298 0.14363441 0.0302821 ]\n",
      " [0.16950716 0.18109997 0.42198905 0.22740385]\n",
      " [0.14301495 0.18087575 0.41894013 0.2571692 ]]\n",
      "Subject 56 Outputs:\n",
      "[[0.5851984  0.18282728 0.16538507 0.06658923]\n",
      " [0.5962998  0.18513805 0.16561046 0.05295158]\n",
      " [0.07362564 0.10055903 0.5871184  0.23869698]\n",
      " [0.1033705  0.11763259 0.5194786  0.2595183 ]]\n",
      "Subject 57 Outputs:\n",
      "[[0.5384505  0.24221784 0.13128519 0.08804649]\n",
      " [0.55936456 0.2606266  0.11840852 0.06160029]\n",
      " [0.03683178 0.13015157 0.35656244 0.47645426]\n",
      " [0.05472375 0.14563283 0.34119007 0.45845336]]\n",
      "Subject 58 Outputs:\n",
      "[[8.9462024e-01 8.2225487e-02 1.7893897e-02 5.2603795e-03]\n",
      " [9.5398438e-01 4.0873103e-02 4.4830767e-03 6.5940019e-04]\n",
      " [2.0265378e-01 2.6582876e-01 3.0270451e-01 2.2881295e-01]\n",
      " [2.8033894e-01 2.7283257e-01 2.5787801e-01 1.8895055e-01]]\n",
      "Subject 59 Outputs:\n",
      "[[0.6157517  0.16084917 0.14460857 0.07879058]\n",
      " [0.62750995 0.16543233 0.14865081 0.05840689]\n",
      " [0.03877728 0.14478937 0.3070416  0.5093918 ]\n",
      " [0.10376254 0.18560801 0.28507283 0.42555663]]\n",
      "Subject 60 Outputs:\n",
      "[[0.5875389  0.15286615 0.16936857 0.09022633]\n",
      " [0.74857956 0.11384888 0.11510057 0.02247105]\n",
      " [0.00699802 0.09682771 0.32120532 0.57496893]\n",
      " [0.03221799 0.14774854 0.32174113 0.49829233]]\n",
      "Subject 61 Outputs:\n",
      "[[0.75532323 0.12454335 0.09508851 0.02504489]\n",
      " [0.84315056 0.09121503 0.05931459 0.00631977]\n",
      " [0.05791165 0.14940766 0.31876376 0.47391698]\n",
      " [0.07102916 0.16457397 0.3038873  0.4605096 ]]\n",
      "Subject 62 Outputs:\n",
      "[[0.6302664  0.2696684  0.09580325 0.00426195]\n",
      " [0.63586813 0.28600603 0.07663568 0.0014901 ]\n",
      " [0.25165078 0.2562003  0.3914625  0.10068639]\n",
      " [0.21308249 0.28907537 0.37151706 0.12632509]]\n",
      "Subject 63 Outputs:\n",
      "[[0.5757351  0.18857878 0.12840033 0.10728579]\n",
      " [0.69983405 0.1880228  0.08083475 0.03130838]\n",
      " [0.21142277 0.18125387 0.27986303 0.32746026]\n",
      " [0.20551105 0.19639671 0.27968794 0.3184043 ]]\n",
      "Subject 64 Outputs:\n",
      "[[0.53263557 0.28136632 0.12789467 0.05810345]\n",
      " [0.57919    0.36206564 0.05282743 0.00591691]\n",
      " [0.22787751 0.232006   0.2892027  0.2509138 ]\n",
      " [0.19542319 0.26507103 0.30640122 0.23310451]]\n",
      "Subject 65 Outputs:\n",
      "[[0.51341945 0.26105684 0.19655184 0.02897194]\n",
      " [0.5442192  0.32221895 0.13160406 0.00195781]\n",
      " [0.11298833 0.28409153 0.46596357 0.13695653]\n",
      " [0.10073593 0.29255196 0.48209482 0.12461731]]\n",
      "Subject 66 Outputs:\n",
      "[[0.4868591  0.1814422  0.17384872 0.15785   ]\n",
      " [0.6067559  0.20734814 0.1295069  0.05638907]\n",
      " [0.08495581 0.15738674 0.27795854 0.4796989 ]\n",
      " [0.09877101 0.17039174 0.2734048  0.45743242]]\n",
      "Subject 67 Outputs:\n",
      "[[0.4929636  0.24916132 0.1377199  0.12015527]\n",
      " [0.58195263 0.30113995 0.07408804 0.0428194 ]\n",
      " [0.03453329 0.20484951 0.3166363  0.4439809 ]\n",
      " [0.05145582 0.23192468 0.3072705  0.40934896]]\n",
      "Subject 68 Outputs:\n",
      "[[0.4844556  0.178459   0.16593084 0.17115465]\n",
      " [0.58174354 0.18617478 0.1268355  0.10524616]\n",
      " [0.05395971 0.16543722 0.4211974  0.35940564]\n",
      " [0.06753973 0.18209416 0.39847282 0.35189337]]\n",
      "Subject 69 Outputs:\n",
      "[[0.7765178  0.1437592  0.05316956 0.02655337]\n",
      " [0.84394497 0.1222622  0.02559658 0.00819619]\n",
      " [0.3170779  0.2859525  0.22887534 0.1680942 ]\n",
      " [0.24476443 0.31968516 0.25816795 0.17738238]]\n",
      "Subject 70 Outputs:\n",
      "[[0.55687773 0.17743327 0.14703497 0.118654  ]\n",
      " [0.5752744  0.18589191 0.14116544 0.09766828]\n",
      " [0.05384161 0.2191042  0.43374243 0.29331186]\n",
      " [0.05289082 0.22246519 0.43405753 0.29058644]]\n",
      "Subject 71 Outputs:\n",
      "[[0.5005568  0.15611237 0.15997458 0.1833562 ]\n",
      " [0.61657673 0.14874518 0.12131185 0.11336615]\n",
      " [0.02245965 0.15574388 0.47456595 0.34723058]\n",
      " [0.02357084 0.15886329 0.46949562 0.3480702 ]]\n",
      "Subject 72 Outputs:\n",
      "[[0.60022056 0.12512158 0.13519901 0.13945878]\n",
      " [0.7755089  0.09377829 0.06914832 0.06156451]\n",
      " [0.0383176  0.09809257 0.4685504  0.39503947]\n",
      " [0.04430293 0.10384385 0.46152285 0.39033028]]\n",
      "Subject 73 Outputs:\n",
      "[[0.5111721  0.09595595 0.24039064 0.15248138]\n",
      " [0.5376816  0.09121468 0.24252412 0.12857968]\n",
      " [0.00931398 0.04403548 0.43579993 0.51085067]\n",
      " [0.01131747 0.04792709 0.4429765  0.49777904]]\n",
      "Subject 74 Outputs:\n",
      "[[0.58321935 0.09747544 0.1763087  0.14299645]\n",
      " [0.63732    0.09388188 0.15965503 0.10914311]\n",
      " [0.02905486 0.07282253 0.37910065 0.5190219 ]\n",
      " [0.0292735  0.07416435 0.38161469 0.5149474 ]]\n",
      "Subject 75 Outputs:\n",
      "[[0.58340096 0.17953292 0.1611445  0.0759217 ]\n",
      " [0.5752335  0.20266238 0.1517068  0.0703973 ]\n",
      " [0.15675603 0.17061374 0.45097774 0.22165251]\n",
      " [0.15963496 0.17710124 0.446228   0.21703577]]\n",
      "Subject 76 Outputs:\n",
      "[[0.46686116 0.18245181 0.20157619 0.1491109 ]\n",
      " [0.45847607 0.2097305  0.1829036  0.14888984]\n",
      " [0.03994572 0.09804387 0.36994046 0.49206996]\n",
      " [0.04325302 0.10206102 0.36854532 0.4861406 ]]\n",
      "Subject 77 Outputs:\n",
      "[[0.73986554 0.13000102 0.08630246 0.04383091]\n",
      " [0.69118166 0.16093671 0.09076864 0.05711306]\n",
      " [0.10612909 0.20538054 0.3889987  0.2994917 ]\n",
      " [0.08205515 0.20225683 0.39686602 0.31882197]]\n",
      "Subject 78 Outputs:\n",
      "[[0.70744187 0.10612158 0.1037574  0.08267921]\n",
      " [0.62562317 0.13184668 0.12011657 0.12241349]\n",
      " [0.02643071 0.18948178 0.31706798 0.46701962]\n",
      " [0.02888549 0.1962176  0.3135147  0.46138218]]\n",
      "Subject 79 Outputs:\n",
      "[[0.52585536 0.13413419 0.24660034 0.09341013]\n",
      " [0.5296312  0.14542009 0.22445233 0.10049634]\n",
      " [0.11262275 0.19938432 0.4070719  0.28092098]\n",
      " [0.13435623 0.20278738 0.39704958 0.26580676]]\n",
      "Subject 80 Outputs:\n",
      "[[0.5539926  0.11887252 0.20306535 0.12406944]\n",
      " [0.5244172  0.13023673 0.19614027 0.14920577]\n",
      " [0.02299835 0.10965095 0.4601874  0.4071633 ]\n",
      " [0.0450011  0.12892021 0.43534648 0.3907321 ]]\n",
      "Subject 81 Outputs:\n",
      "[[0.8373162  0.11695488 0.03701866 0.00871021]\n",
      " [0.8099328  0.14021926 0.03748683 0.01236111]\n",
      " [0.1405504  0.30254546 0.40247625 0.15442789]\n",
      " [0.1496759  0.29299662 0.3886784  0.1686491 ]]\n",
      "Subject 82 Outputs:\n",
      "[[0.879001   0.10065026 0.01368695 0.00666186]\n",
      " [0.8944914  0.09373266 0.00742967 0.00434634]\n",
      " [0.06804588 0.19544202 0.37939045 0.35712168]\n",
      " [0.10332785 0.22313789 0.3519171  0.3216172 ]]\n",
      "Subject 83 Outputs:\n",
      "[[0.54712266 0.22726344 0.1426133  0.08300056]\n",
      " [0.5474874  0.2436855  0.12978779 0.07903932]\n",
      " [0.01607786 0.26400188 0.56310946 0.1568108 ]\n",
      " [0.02375058 0.2788377  0.5371089  0.16030276]]\n",
      "Subject 84 Outputs:\n",
      "[[0.7768507  0.1527017  0.05516731 0.01528027]\n",
      " [0.74721515 0.17317933 0.06068762 0.0189179 ]\n",
      " [0.06368721 0.2652751  0.55144244 0.11959523]\n",
      " [0.08673651 0.28233454 0.5124406  0.11848834]]\n",
      "Subject 85 Outputs:\n",
      "[[0.7378443  0.16532806 0.07161096 0.02521676]\n",
      " [0.7322386  0.17308259 0.07039566 0.02428311]\n",
      " [0.04725593 0.4283858  0.30604395 0.2183143 ]\n",
      " [0.06750146 0.42749813 0.29849905 0.2065014 ]]\n",
      "Subject 86 Outputs:\n",
      "[[0.68341094 0.10890912 0.14567006 0.06200989]\n",
      " [0.63787323 0.123596   0.1649594  0.07357131]\n",
      " [0.08495606 0.24681227 0.26846957 0.39976215]\n",
      " [0.10225992 0.24411592 0.2698625  0.38376167]]\n",
      "Subject 87 Outputs:\n",
      "[[0.5005523  0.17229846 0.27135342 0.05579579]\n",
      " [0.5233106  0.16171135 0.28708023 0.02789792]\n",
      " [0.10688116 0.22302149 0.2367116  0.43338567]\n",
      " [0.13241059 0.22179149 0.24158831 0.4042096 ]]\n",
      "Subject 88 Outputs:\n",
      "[[0.42831817 0.15043148 0.2285705  0.19267976]\n",
      " [0.691901   0.14318447 0.13802192 0.02689262]\n",
      " [0.0659377  0.18325737 0.32576147 0.42504352]\n",
      " [0.15302451 0.1925079  0.3113271  0.34314045]]\n",
      "Subject 89 Outputs:\n",
      "[[0.4977235  0.21429649 0.17067522 0.11730476]\n",
      " [0.6265203  0.27719402 0.06537506 0.03091061]\n",
      " [0.0681586  0.28359038 0.34495392 0.30329704]\n",
      " [0.10817066 0.28495896 0.32688966 0.27998072]]\n",
      "Subject 90 Outputs:\n",
      "[[0.5534411  0.20712005 0.1466285  0.09281036]\n",
      " [0.5926816  0.21901913 0.12102762 0.06727167]\n",
      " [0.10123289 0.22036785 0.3210308  0.3573685 ]\n",
      " [0.16035868 0.22626953 0.30173776 0.311634  ]]\n",
      "Subject 91 Outputs:\n",
      "[[0.58802134 0.2596356  0.08829432 0.06404881]\n",
      " [0.59376496 0.27467176 0.07365026 0.05791299]\n",
      " [0.06869893 0.30390316 0.2945641  0.33283377]\n",
      " [0.13572341 0.2976923  0.27115005 0.2954342 ]]\n",
      "Subject 92 Outputs:\n",
      "[[0.71380234 0.11404793 0.10537066 0.0667791 ]\n",
      " [0.6146859  0.13793418 0.13450937 0.1128706 ]\n",
      " [0.07947721 0.18251185 0.40845042 0.32956046]\n",
      " [0.13164915 0.18471588 0.37061733 0.3130176 ]]\n",
      "Subject 93 Outputs:\n",
      "[[0.54573816 0.13349646 0.19682983 0.12393559]\n",
      " [0.5395698  0.14335029 0.18205151 0.13502835]\n",
      " [0.03216616 0.16137724 0.4679425  0.33851406]\n",
      " [0.10754429 0.17832874 0.39484227 0.3192847 ]]\n",
      "Subject 94 Outputs:\n",
      "[[0.7471808  0.10959271 0.11632738 0.02689909]\n",
      " [0.7590125  0.12136073 0.09366421 0.02596257]\n",
      " [0.08851478 0.25943625 0.38956386 0.26248512]\n",
      " [0.20195821 0.2438961  0.33709893 0.21704672]]\n",
      "Subject 95 Outputs:\n",
      "[[0.6043592  0.06555592 0.26609913 0.06398573]\n",
      " [0.686345   0.05830667 0.21783134 0.03751695]\n",
      " [0.01919853 0.15545624 0.41554472 0.40980044]\n",
      " [0.06200348 0.17280711 0.39093098 0.37425846]]\n",
      "Subject 96 Outputs:\n",
      "[[0.5531328  0.05665435 0.3333881  0.05682477]\n",
      " [0.54407173 0.08004143 0.30004397 0.07584289]\n",
      " [0.03495332 0.15884155 0.34275874 0.4634464 ]\n",
      " [0.04638294 0.16335313 0.34211716 0.44814676]]\n",
      "Subject 97 Outputs:\n",
      "[[0.8137875  0.0806377  0.09666722 0.00890754]\n",
      " [0.6661068  0.1557129  0.14654785 0.03163254]\n",
      " [0.04353257 0.23748927 0.5454664  0.17351179]\n",
      " [0.02834338 0.25396854 0.54854196 0.16914617]]\n",
      "Subject 98 Outputs:\n",
      "[[0.56993234 0.18490086 0.19814894 0.04701785]\n",
      " [0.52230304 0.22591129 0.19600616 0.05577952]\n",
      " [0.38039488 0.20247906 0.2744324  0.14269356]\n",
      " [0.2387435  0.27728042 0.3381667  0.1458094 ]]\n",
      "Subject 99 Outputs:\n",
      "[[0.53182155 0.1608789  0.19257714 0.11472243]\n",
      " [0.5534802  0.18082984 0.16927521 0.09641478]\n",
      " [0.04771253 0.17497426 0.44002423 0.33728895]\n",
      " [0.07099754 0.18348071 0.4184232  0.32709855]]\n",
      "Subject 100 Outputs:\n",
      "[[0.7234294  0.16049561 0.08649054 0.02958448]\n",
      " [0.71962804 0.1707536  0.08265699 0.02696134]\n",
      " [0.05281997 0.20494859 0.26121205 0.4810194 ]\n",
      " [0.173648   0.23779806 0.25545928 0.33309463]]\n",
      "Subject 101 Outputs:\n",
      "[[0.59687513 0.198158   0.13776009 0.06720677]\n",
      " [0.6060707  0.21241981 0.13089366 0.05061578]\n",
      " [0.01157905 0.13755448 0.31943873 0.53142774]\n",
      " [0.03279502 0.16451587 0.32659912 0.47608995]]\n",
      "Subject 102 Outputs:\n",
      "[[0.66391426 0.16579396 0.14572693 0.02456478]\n",
      " [0.6613681  0.1737285  0.14603807 0.01886534]\n",
      " [0.07288994 0.20662874 0.31662118 0.40386018]\n",
      " [0.11599914 0.22662483 0.32035428 0.33702174]]\n",
      "Subject 103 Outputs:\n",
      "[[0.5576476  0.26428366 0.14142561 0.03664318]\n",
      " [0.5511177  0.281214   0.14328763 0.02438061]\n",
      " [0.07368828 0.276741   0.34125417 0.30831653]\n",
      " [0.09390047 0.31848916 0.34076902 0.2468413 ]]\n",
      "Subject 104 Outputs:\n",
      "[[0.66541374 0.29117906 0.03598527 0.00742198]\n",
      " [0.6398921  0.33426106 0.02334273 0.00250409]\n",
      " [0.05858401 0.3185375  0.4106345  0.21224399]\n",
      " [0.05757037 0.35403028 0.3948756  0.19352381]]\n",
      "Subject 105 Outputs:\n",
      "[[0.698725   0.21688963 0.05557252 0.02881283]\n",
      " [0.6158669  0.23903212 0.09613788 0.04896308]\n",
      " [0.0166774  0.2518643  0.47687116 0.25458714]\n",
      " [0.01980463 0.26324472 0.45946884 0.25748187]]\n",
      "Subject 106 Outputs:\n",
      "[[0.7104074  0.23306331 0.03078792 0.02574141]\n",
      " [0.6080683  0.23469138 0.08557086 0.07166941]\n",
      " [0.05581239 0.2881553  0.3188488  0.3371835 ]\n",
      " [0.09303218 0.305019   0.29039168 0.31155717]]\n",
      "Subject 107 Outputs:\n",
      "[[0.7113057  0.20185508 0.06455673 0.0222825 ]\n",
      " [0.6192332  0.23197033 0.10457259 0.04422389]\n",
      " [0.01582891 0.30211988 0.3367768  0.34527442]\n",
      " [0.03182549 0.28839493 0.3277375  0.35204202]]\n",
      "Subject 108 Outputs:\n",
      "[[0.7646618  0.1182673  0.10852855 0.00854237]\n",
      " [0.7062853  0.14401798 0.13559557 0.0141011 ]\n",
      " [0.01215085 0.48221365 0.37912187 0.12651359]\n",
      " [0.03626487 0.4232142  0.37468103 0.16583988]]\n",
      "Subject 109 Outputs:\n",
      "[[0.81169665 0.09707475 0.05681083 0.03441788]\n",
      " [0.7299375  0.13065661 0.08756167 0.05184415]\n",
      " [0.1103203  0.39468354 0.29621065 0.19878551]\n",
      " [0.14027911 0.35001132 0.29163554 0.21807407]]\n",
      "Subject 110 Outputs:\n",
      "[[0.6573182  0.09126984 0.1327699  0.11864199]\n",
      " [0.6211131  0.11374773 0.15318882 0.11195034]\n",
      " [0.02062383 0.17611583 0.2293903  0.57387006]\n",
      " [0.03668907 0.18106923 0.24448735 0.53775436]]\n",
      "Subject 111 Outputs:\n",
      "[[0.67911714 0.0830726  0.12867595 0.10913436]\n",
      " [0.57078105 0.12024012 0.16563891 0.14333995]\n",
      " [0.00431848 0.10477309 0.10286915 0.78803927]\n",
      " [0.01433179 0.12944138 0.14267904 0.7135478 ]]\n",
      "Subject 112 Outputs:\n",
      "[[0.6285112  0.10897917 0.14714184 0.11536783]\n",
      " [0.7728438  0.08848736 0.0907416  0.0479273 ]\n",
      " [0.0868849  0.2074945  0.33462402 0.3709965 ]\n",
      " [0.17317598 0.20612383 0.3124541  0.30824605]]\n",
      "Subject 113 Outputs:\n",
      "[[0.6866684  0.18823233 0.09920963 0.02588969]\n",
      " [0.71956116 0.191336   0.07696979 0.01213303]\n",
      " [0.10022639 0.32348138 0.40961507 0.16667713]\n",
      " [0.07516909 0.32943064 0.42730382 0.16809645]]\n",
      "Subject 114 Outputs:\n",
      "[[0.8103946  0.08476226 0.0954222  0.00942105]\n",
      " [0.8130067  0.08154713 0.0984335  0.00701272]\n",
      " [0.05328201 0.26278263 0.53580076 0.14813459]\n",
      " [0.07615592 0.26306397 0.5118522  0.14892794]]\n",
      "Subject 115 Outputs:\n",
      "[[0.5117632  0.1505504  0.19090903 0.14677736]\n",
      " [0.59222555 0.15033221 0.18287523 0.07456703]\n",
      " [0.01486374 0.1520352  0.31492788 0.51817316]\n",
      " [0.02564211 0.162462   0.32583195 0.48606393]]\n",
      "Subject 116 Outputs:\n",
      "[[0.5084494  0.19629548 0.15489286 0.14036229]\n",
      " [0.5685791  0.17295924 0.16820066 0.09026093]\n",
      " [0.01066352 0.27085248 0.26521984 0.4532642 ]\n",
      " [0.01714353 0.26427743 0.27313653 0.44544256]]\n",
      "Subject 117 Outputs:\n",
      "[[0.5680654  0.22375068 0.14983141 0.05835248]\n",
      " [0.652332   0.15231559 0.16878612 0.02656631]\n",
      " [0.06604322 0.21488501 0.35759467 0.36147705]\n",
      " [0.14298223 0.21840039 0.3522901  0.28632727]]\n",
      "Subject 118 Outputs:\n",
      "[[0.4973054  0.2261523  0.10137583 0.17516655]\n",
      " [0.635582   0.2653679  0.05796894 0.04108117]\n",
      " [0.07812235 0.15502499 0.29201007 0.47484264]\n",
      " [0.08478669 0.15690467 0.29195118 0.46635753]]\n",
      "Subject 119 Outputs:\n",
      "[[0.50064963 0.36594906 0.06977033 0.06363104]\n",
      " [0.5534874  0.42052263 0.02097546 0.00501453]\n",
      " [0.02626048 0.12221397 0.4279058  0.42361975]\n",
      " [0.03129701 0.1286442  0.42689583 0.41316295]]\n",
      "Subject 120 Outputs:\n",
      "[[0.6142203  0.24467638 0.09723375 0.0438695 ]\n",
      " [0.6850853  0.21643053 0.08851961 0.00996452]\n",
      " [0.09361248 0.17217167 0.31853798 0.41567788]\n",
      " [0.09285481 0.17192    0.31869808 0.41652703]]\n",
      "Subject 121 Outputs:\n",
      "[[0.48788667 0.22545534 0.20250121 0.08415681]\n",
      " [0.4944765  0.19981897 0.24552517 0.06017932]\n",
      " [0.03720588 0.1988027  0.3135431  0.45044836]\n",
      " [0.03659425 0.19845119 0.3135816  0.45137304]]\n",
      "Subject 122 Outputs:\n",
      "[[0.5882835  0.14473931 0.16385344 0.10312381]\n",
      " [0.5175606  0.13210087 0.19762595 0.15271252]\n",
      " [0.04629188 0.09823246 0.27907813 0.57639754]\n",
      " [0.04503419 0.09753116 0.2785344  0.5789002 ]]\n",
      "Subject 123 Outputs:\n",
      "[[0.6566558  0.14798228 0.17446215 0.02089979]\n",
      " [0.6453328  0.14438061 0.18477377 0.02551286]\n",
      " [0.10005391 0.19206594 0.4406478  0.2672324 ]\n",
      " [0.09616743 0.19074127 0.44164914 0.27144217]]\n",
      "Subject 124 Outputs:\n",
      "[[0.7392077  0.1737113  0.07825869 0.00882231]\n",
      " [0.7568175  0.16110912 0.07513343 0.00693986]\n",
      " [0.03580013 0.22806266 0.35434967 0.38178754]\n",
      " [0.03522079 0.22735353 0.35446438 0.3829613 ]]\n",
      "Subject 125 Outputs:\n",
      "[[0.40002698 0.4072635  0.15269886 0.04001065]\n",
      " [0.4292879  0.31590173 0.18102898 0.0737813 ]\n",
      " [0.07514853 0.27907783 0.4198964  0.22587723]\n",
      " [0.07365613 0.27844152 0.42135116 0.2265512 ]]\n",
      "Subject 126 Outputs:\n",
      "[[0.69294393 0.17851576 0.1112424  0.01729785]\n",
      " [0.65619874 0.18278669 0.13065554 0.03035905]\n",
      " [0.16187061 0.2855376  0.36190626 0.19068547]\n",
      " [0.16053052 0.28594533 0.3626226  0.19090149]]\n",
      "Subject 127 Outputs:\n",
      "[[0.78005624 0.12327007 0.06016863 0.0365051 ]\n",
      " [0.77946544 0.12905556 0.05807302 0.03340597]\n",
      " [0.09238525 0.24019298 0.33518657 0.33223516]\n",
      " [0.09238525 0.24019298 0.33518657 0.33223516]]\n",
      "Subject 128 Outputs:\n",
      "[[0.6830195  0.23925862 0.05575742 0.02196444]\n",
      " [0.6792589  0.23785019 0.0600692  0.02282177]\n",
      " [0.03020452 0.22590092 0.3841882  0.3597063 ]\n",
      " [0.03020452 0.22590092 0.3841882  0.3597063 ]]\n",
      "Subject 129 Outputs:\n",
      "[[0.5681693  0.2820069  0.12632385 0.0235    ]\n",
      " [0.5671531  0.27399373 0.13396583 0.02488737]\n",
      " [0.01345391 0.16343756 0.36722124 0.45588726]\n",
      " [0.01345391 0.16343756 0.36722124 0.45588726]]\n",
      "Subject 130 Outputs:\n",
      "[[0.5822871  0.28812483 0.10100742 0.02858068]\n",
      " [0.54700464 0.2516966  0.1391251  0.06217362]\n",
      " [0.03925711 0.2149193  0.2460404  0.49978325]\n",
      " [0.03925711 0.2149193  0.2460404  0.49978325]]\n",
      "Subject 131 Outputs:\n",
      "[[0.7813012  0.1073188  0.09071658 0.02066347]\n",
      " [0.72205055 0.11827125 0.11913005 0.04054809]\n",
      " [0.23293321 0.17427848 0.2856489  0.30713943]\n",
      " [0.23293321 0.17427848 0.2856489  0.30713943]]\n",
      "Subject 132 Outputs:\n",
      "[[0.4662302  0.34575027 0.16825584 0.01976371]\n",
      " [0.5024995  0.30910346 0.16620994 0.02218716]\n",
      " [0.07238875 0.15456603 0.3561997  0.41684547]\n",
      " [0.07238875 0.15456603 0.3561997  0.41684547]]\n",
      "Subject 133 Outputs:\n",
      "[[0.6504168  0.2167243  0.10806916 0.02478983]\n",
      " [0.69294435 0.15883781 0.10998042 0.0382375 ]\n",
      " [0.04892913 0.12469289 0.469411   0.356967  ]\n",
      " [0.04892913 0.12469289 0.469411   0.356967  ]]\n",
      "Subject 134 Outputs:\n",
      "[[0.79242045 0.16425036 0.03755655 0.00577256]\n",
      " [0.77159065 0.15914665 0.05492916 0.01433353]\n",
      " [0.02831902 0.19503535 0.2867085  0.48993716]\n",
      " [0.02831902 0.19503535 0.2867085  0.48993716]]\n",
      "Subject 135 Outputs:\n",
      "[[0.7800942  0.14427447 0.06209655 0.0135349 ]\n",
      " [0.64735955 0.14711402 0.13260677 0.07291961]\n",
      " [0.0529352  0.15668163 0.3485899  0.44179326]\n",
      " [0.0529352  0.15668163 0.3485899  0.44179326]]\n",
      "Subject 136 Outputs:\n",
      "[[0.6671151  0.28380018 0.04056692 0.00851784]\n",
      " [0.5907488  0.21566947 0.1246672  0.06891447]\n",
      " [0.03083351 0.33692732 0.36089894 0.27134016]\n",
      " [0.03083351 0.33692732 0.36089894 0.27134016]]\n",
      "Subject 137 Outputs:\n",
      "[[0.62225485 0.30403084 0.06525446 0.00845985]\n",
      " [0.61895496 0.23887855 0.10653198 0.03563451]\n",
      " [0.05278254 0.28060612 0.27105856 0.39555278]\n",
      " [0.05278254 0.28060612 0.27105856 0.39555278]]\n",
      "Subject 138 Outputs:\n",
      "[[0.44224662 0.29945382 0.11554714 0.14275241]\n",
      " [0.0766793  0.24451198 0.20448467 0.47432408]\n",
      " [0.01707533 0.21309794 0.17939678 0.5904299 ]\n",
      " [0.01707533 0.21309794 0.17939678 0.5904299 ]]\n",
      "Subject 139 Outputs:\n",
      "[[0.6758496  0.23464097 0.0496422  0.03986725]\n",
      " [0.6013623  0.1934703  0.10269958 0.10246782]\n",
      " [0.04890332 0.24044678 0.3079809  0.402669  ]\n",
      " [0.04890332 0.24044678 0.3079809  0.402669  ]]\n",
      "Subject 140 Outputs:\n",
      "[[0.43422064 0.3154029  0.16125311 0.08912331]\n",
      " [0.46299294 0.31117755 0.1449753  0.0808542 ]\n",
      " [0.01941862 0.22940737 0.44140074 0.3097733 ]\n",
      " [0.01941862 0.22940737 0.44140074 0.3097733 ]]\n",
      "Subject 141 Outputs:\n",
      "[[0.6314193  0.16499735 0.12860808 0.07497521]\n",
      " [0.61027163 0.15277977 0.13821976 0.09872882]\n",
      " [0.05987664 0.12476862 0.21983163 0.5955231 ]\n",
      " [0.05980094 0.12473356 0.21980152 0.595664  ]]\n",
      "Subject 142 Outputs:\n",
      "[[0.7519086  0.15876675 0.07170346 0.0176212 ]\n",
      " [0.6808942  0.16017509 0.11232263 0.0466081 ]\n",
      " [0.16964775 0.20074749 0.31114694 0.31845778]\n",
      " [0.16554989 0.20037788 0.31244433 0.32162786]]\n",
      "Subject 143 Outputs:\n",
      "[[0.66472864 0.24058168 0.08638988 0.00829982]\n",
      " [0.63255817 0.23154888 0.11581864 0.02007438]\n",
      " [0.10278545 0.31402117 0.34292436 0.24026906]\n",
      " [0.10278545 0.31402117 0.34292436 0.24026906]]\n",
      "Subject 144 Outputs:\n",
      "[[0.7597594  0.15688233 0.07423999 0.0091183 ]\n",
      " [0.7520408  0.15399852 0.08093569 0.01302503]\n",
      " [0.09857127 0.18558659 0.34388667 0.37195548]\n",
      " [0.09857127 0.18558659 0.34388667 0.37195548]]\n",
      "Subject 145 Outputs:\n",
      "[[0.63471407 0.29686093 0.06574629 0.00267872]\n",
      " [0.6333568  0.30597457 0.05825981 0.00240872]\n",
      " [0.18584968 0.33305177 0.32823747 0.15286109]\n",
      " [0.10920163 0.33795294 0.36604193 0.18680345]]\n",
      "Subject 146 Outputs:\n",
      "[[0.6416794  0.2241122  0.12638614 0.00782218]\n",
      " [0.65572345 0.22657356 0.11078059 0.00692233]\n",
      " [0.04892477 0.22389361 0.36849555 0.35868606]\n",
      " [0.04893996 0.22388417 0.3684777  0.3586982 ]]\n",
      "Subject 147 Outputs:\n",
      "[[0.61476624 0.14014904 0.1833853  0.06169941]\n",
      " [0.58930373 0.14596486 0.16646065 0.09827076]\n",
      " [0.05568815 0.17550938 0.24085395 0.52794856]\n",
      " [0.05120156 0.1732264  0.23856464 0.53700745]]\n",
      "Subject 148 Outputs:\n",
      "[[0.5266347  0.3053672  0.14774805 0.02024999]\n",
      " [0.5420454  0.2935463  0.13053322 0.033875  ]\n",
      " [0.05011422 0.23374906 0.2314891  0.48464757]\n",
      " [0.02532429 0.22463739 0.21349563 0.5365427 ]]\n",
      "Subject 149 Outputs:\n",
      "[[0.8643718  0.07200689 0.0627531  0.00086821]\n",
      " [0.8417449  0.08939676 0.06691124 0.00194705]\n",
      " [0.20798324 0.27626836 0.34792444 0.16782399]\n",
      " [0.16603525 0.30540568 0.37695247 0.15160659]]\n"
     ]
    }
   ],
   "source": [
    "model = BCModel()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-5)\n",
    "\n",
    "num_samples = 45\n",
    "\n",
    "all_subjects_outputs = {}\n",
    "\n",
    "for i, (states, actions) in enumerate(zip(states_dataset.values, actions_dataset.values)):\n",
    "    states_expanded = np.column_stack((states, np.roll(states, 1)))\n",
    "    states_expanded[0, 1] = states_expanded[0, 0]  # First state's previous state set to itself\n",
    "    \n",
    "    # Convert to tensors\n",
    "    states_tensor = torch.tensor(states_expanded, dtype=torch.float32)\n",
    "    actions_tensor = torch.tensor(actions, dtype=torch.long)\n",
    "    \n",
    "    # Create DataLoader\n",
    "    dataset = TensorDataset(states_tensor, actions_tensor)\n",
    "    loader = DataLoader(dataset, batch_size=10, shuffle=True)\n",
    "    \n",
    "    # Training loop\n",
    "    model.train()\n",
    "    for epoch in range(40):\n",
    "        running_loss = 0.0\n",
    "        for inputs, labels in loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = -torch.sum(torch.log(outputs.gather(1, labels.unsqueeze(1)).squeeze(1)))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "\n",
    "    # Evaluation loop for custom inputs\n",
    "    model.eval()\n",
    "    custom_inputs = torch.tensor([[0, 0], [0, 1], [1, 0], [1, 1]], dtype=torch.float32)\n",
    "    with torch.no_grad():\n",
    "        custom_outputs = model(custom_inputs)\n",
    "        all_subjects_outputs[i] = custom_outputs.numpy()\n",
    "\n",
    "    # Regular evaluation loop\n",
    "    total_deviation = 0.0\n",
    "    evaluation_loss = 0.0\n",
    "    num_samples = len(states)\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in loader:\n",
    "            outputs = model(inputs)\n",
    "            _, predicted_classes = torch.max(outputs, 1)\n",
    "            deviation = torch.abs(predicted_classes.float() - labels.float())\n",
    "            total_deviation += deviation.sum().item()\n",
    "            eval_loss = -torch.sum(torch.log(outputs.gather(1, labels.unsqueeze(1)).squeeze(1)))\n",
    "            evaluation_loss += eval_loss.item()\n",
    "\n",
    "    average_deviation = total_deviation / num_samples\n",
    "    average_evaluation_loss = evaluation_loss / num_samples\n",
    "    print(f\"Subject {i} - Average Deviation: {average_deviation}\")\n",
    "    print(f\"Subject {i} - Evaluation Loss: {average_evaluation_loss}\")\n",
    "\n",
    "# Print model outputs for specific inputs after all subjects are processed\n",
    "print(\"Model outputs for specific inputs across all subjects:\")\n",
    "for subject, outputs in all_subjects_outputs.items():\n",
    "    print(f\"Subject {subject} Outputs:\")\n",
    "    print(outputs)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-14T00:36:08.668149700Z",
     "start_time": "2024-06-14T00:35:06.478887100Z"
    }
   },
   "id": "65529c0dd06b4cad",
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "{0: [array([0.28795505, 0.16379717, 0.20148192, 0.3467658 ], dtype=float32),\n  array([0.47700673, 0.11373572, 0.17789644, 0.23136109], dtype=float32),\n  array([0.11525543, 0.14385   , 0.1609354 , 0.57995915], dtype=float32),\n  array([0.12320498, 0.11575945, 0.16230999, 0.5987256 ], dtype=float32)],\n 1: [array([0.31164536, 0.32230547, 0.21617943, 0.14986981], dtype=float32),\n  array([0.36244816, 0.3256212 , 0.19478193, 0.11714873], dtype=float32),\n  array([0.07984774, 0.24552882, 0.37149543, 0.303128  ], dtype=float32),\n  array([0.09699935, 0.26569858, 0.36706525, 0.27023688], dtype=float32)],\n 2: [array([0.67985535, 0.176637  , 0.09162865, 0.05187899], dtype=float32),\n  array([0.7725288 , 0.13232285, 0.0689183 , 0.02622996], dtype=float32),\n  array([0.16021807, 0.20068312, 0.31099933, 0.32809952], dtype=float32),\n  array([0.10243398, 0.17131346, 0.37347448, 0.35277808], dtype=float32)],\n 3: [array([0.6007747 , 0.20085366, 0.05791166, 0.14046003], dtype=float32),\n  array([0.70639646, 0.1565903 , 0.04388369, 0.09312954], dtype=float32),\n  array([0.11600015, 0.18026294, 0.27221265, 0.4315243 ], dtype=float32),\n  array([0.06054485, 0.13946974, 0.38721573, 0.4127697 ], dtype=float32)],\n 4: [array([0.59375733, 0.22117585, 0.09831078, 0.08675607], dtype=float32),\n  array([0.79278666, 0.12772492, 0.04974888, 0.02973957], dtype=float32),\n  array([0.10180057, 0.23983271, 0.29336026, 0.36500648], dtype=float32),\n  array([0.16674727, 0.24531314, 0.29715207, 0.29078752], dtype=float32)],\n 5: [array([0.52139723, 0.34720445, 0.08225492, 0.04914349], dtype=float32),\n  array([0.6271973 , 0.323277  , 0.035636  , 0.01388964], dtype=float32),\n  array([0.04179654, 0.17219312, 0.47690287, 0.30910745], dtype=float32),\n  array([0.06828501, 0.20952497, 0.4441856 , 0.27800444], dtype=float32)],\n 6: [array([0.7931063 , 0.14985299, 0.04072284, 0.01631779], dtype=float32),\n  array([0.8377542 , 0.12366548, 0.03017906, 0.00840126], dtype=float32),\n  array([0.18739751, 0.23193076, 0.25326344, 0.3274083 ], dtype=float32),\n  array([0.1378889 , 0.22062059, 0.29822785, 0.34326267], dtype=float32)],\n 7: [array([0.41252026, 0.26509234, 0.28446046, 0.03792689], dtype=float32),\n  array([0.40885505, 0.26359147, 0.29948375, 0.02806978], dtype=float32),\n  array([0.07402349, 0.14563747, 0.49936262, 0.28097638], dtype=float32),\n  array([0.08029099, 0.15146522, 0.5066863 , 0.26155752], dtype=float32)],\n 8: [array([0.6224164 , 0.25562087, 0.08566944, 0.03629327], dtype=float32),\n  array([0.6645304 , 0.26544395, 0.05591946, 0.01410613], dtype=float32),\n  array([0.28500286, 0.18477184, 0.23621695, 0.29400837], dtype=float32),\n  array([0.28396067, 0.18627137, 0.24353677, 0.28623122], dtype=float32)],\n 9: [array([0.87480915, 0.09722706, 0.02229233, 0.00567144], dtype=float32),\n  array([9.5428228e-01, 4.1311745e-02, 3.9702365e-03, 4.3577846e-04],\n        dtype=float32),\n  array([0.27617747, 0.1858549 , 0.26110828, 0.27685934], dtype=float32),\n  array([0.40968645, 0.21882027, 0.20166342, 0.16982977], dtype=float32)],\n 10: [array([0.8132287 , 0.10739782, 0.07134337, 0.00803012], dtype=float32),\n  array([9.3095392e-01, 4.4268493e-02, 2.3937957e-02, 8.3971507e-04],\n        dtype=float32),\n  array([0.0381898 , 0.10203495, 0.5839165 , 0.27585876], dtype=float32),\n  array([0.05996366, 0.1280986 , 0.592814  , 0.21912377], dtype=float32)],\n 11: [array([0.7580745 , 0.18169345, 0.05322316, 0.0070089 ], dtype=float32),\n  array([0.7384795 , 0.20128152, 0.05639985, 0.00383912], dtype=float32),\n  array([0.162664  , 0.22494167, 0.22905616, 0.38333818], dtype=float32),\n  array([0.16555177, 0.23864959, 0.27116027, 0.3246384 ], dtype=float32)],\n 12: [array([0.54582185, 0.33790612, 0.07886738, 0.03740467], dtype=float32),\n  array([0.50274944, 0.3444035 , 0.11223993, 0.04060705], dtype=float32),\n  array([0.03921986, 0.10383141, 0.26236662, 0.5945821 ], dtype=float32),\n  array([0.05878149, 0.12808692, 0.3429623 , 0.4701694 ], dtype=float32)],\n 13: [array([0.6544176 , 0.2833072 , 0.03266237, 0.02961291], dtype=float32),\n  array([0.6463641 , 0.3059067 , 0.03071209, 0.01701718], dtype=float32),\n  array([0.05488825, 0.27400225, 0.37390792, 0.29720154], dtype=float32),\n  array([0.078199  , 0.27748105, 0.4037335 , 0.2405864 ], dtype=float32)],\n 14: [array([0.62279963, 0.14398175, 0.0647074 , 0.16851118], dtype=float32),\n  array([0.5665718 , 0.1801385 , 0.08394739, 0.16934228], dtype=float32),\n  array([0.0132321 , 0.11370111, 0.37728065, 0.49578613], dtype=float32),\n  array([0.02230577, 0.14364491, 0.40800595, 0.42604336], dtype=float32)],\n 15: [array([0.7720971 , 0.11722716, 0.04340647, 0.06726918], dtype=float32),\n  array([0.693866  , 0.1707875 , 0.06540292, 0.06994355], dtype=float32),\n  array([0.07483836, 0.29814154, 0.37058762, 0.25643247], dtype=float32),\n  array([0.13463545, 0.28138983, 0.34132054, 0.24265417], dtype=float32)],\n 16: [array([0.5707684 , 0.18868199, 0.12617758, 0.11437208], dtype=float32),\n  array([0.8023199 , 0.12514406, 0.04409548, 0.0284406 ], dtype=float32),\n  array([0.07044285, 0.2730686 , 0.37914044, 0.2773481 ], dtype=float32),\n  array([0.10900873, 0.26888943, 0.358703  , 0.26339895], dtype=float32)],\n 17: [array([0.50947016, 0.21564785, 0.13161404, 0.14326793], dtype=float32),\n  array([0.6014181 , 0.22746505, 0.09124369, 0.07987325], dtype=float32),\n  array([0.04385924, 0.18402834, 0.47929892, 0.2928135 ], dtype=float32),\n  array([0.04937905, 0.18612473, 0.48099178, 0.28350446], dtype=float32)],\n 18: [array([0.79851866, 0.14321646, 0.02845674, 0.0298082 ], dtype=float32),\n  array([0.9057146 , 0.08366937, 0.00576865, 0.00484739], dtype=float32),\n  array([0.01881462, 0.23330598, 0.44718298, 0.30069637], dtype=float32),\n  array([0.02630356, 0.24919225, 0.44601333, 0.27849084], dtype=float32)],\n 19: [array([0.6012347 , 0.24925151, 0.11155307, 0.03796076], dtype=float32),\n  array([0.6842396 , 0.23492993, 0.06903856, 0.0117918 ], dtype=float32),\n  array([0.00749388, 0.18265471, 0.4279812 , 0.38187015], dtype=float32),\n  array([0.01021658, 0.20488386, 0.4435706 , 0.34132895], dtype=float32)],\n 20: [array([0.6913302 , 0.22433496, 0.03968136, 0.04465347], dtype=float32),\n  array([0.7269882 , 0.21512292, 0.02884931, 0.02903959], dtype=float32),\n  array([0.0831393 , 0.22293594, 0.2439126 , 0.4500121 ], dtype=float32),\n  array([0.10598344, 0.24375841, 0.27252135, 0.37773684], dtype=float32)],\n 21: [array([0.67709064, 0.23797922, 0.06230165, 0.02262851], dtype=float32),\n  array([0.788888  , 0.16699535, 0.0356384 , 0.00847831], dtype=float32),\n  array([0.04812847, 0.21300264, 0.41717365, 0.3216952 ], dtype=float32),\n  array([0.10149469, 0.24166785, 0.42140147, 0.23543598], dtype=float32)],\n 22: [array([0.5310509 , 0.2973366 , 0.08635452, 0.08525801], dtype=float32),\n  array([0.52377075, 0.2962316 , 0.09731769, 0.08267996], dtype=float32),\n  array([0.07327949, 0.25394127, 0.40695223, 0.26582706], dtype=float32),\n  array([0.08903452, 0.2521529 , 0.42795444, 0.23085815], dtype=float32)],\n 23: [array([0.5910166 , 0.29108936, 0.02897153, 0.08892254], dtype=float32),\n  array([0.56685686, 0.28993493, 0.0400877 , 0.10312055], dtype=float32),\n  array([0.098805  , 0.2967097 , 0.28722623, 0.31725904], dtype=float32),\n  array([0.23020805, 0.2878151 , 0.2508957 , 0.23108117], dtype=float32)],\n 24: [array([0.59886867, 0.25321728, 0.06919195, 0.07872209], dtype=float32),\n  array([0.570855  , 0.23987852, 0.08746111, 0.10180531], dtype=float32),\n  array([0.06930713, 0.24453679, 0.3479052 , 0.33825088], dtype=float32),\n  array([0.27974233, 0.23783858, 0.2548991 , 0.22752003], dtype=float32)],\n 25: [array([0.45214644, 0.21906921, 0.21838489, 0.11039944], dtype=float32),\n  array([0.45635957, 0.20353264, 0.23030563, 0.10980224], dtype=float32),\n  array([0.00568964, 0.12023607, 0.48060128, 0.39347303], dtype=float32),\n  array([0.02947943, 0.16548309, 0.4664993 , 0.33853814], dtype=float32)],\n 26: [array([0.43632403, 0.28530192, 0.19884656, 0.07952743], dtype=float32),\n  array([0.44292015, 0.29257038, 0.20223227, 0.0622772 ], dtype=float32),\n  array([0.14169492, 0.28620404, 0.28816336, 0.2839377 ], dtype=float32),\n  array([0.21807952, 0.28771812, 0.29397982, 0.20022257], dtype=float32)],\n 27: [array([0.7261301 , 0.17295985, 0.08024176, 0.0206683 ], dtype=float32),\n  array([0.80709815, 0.13559578, 0.04902609, 0.00827995], dtype=float32),\n  array([0.29854593, 0.24149722, 0.26107457, 0.19888228], dtype=float32),\n  array([0.22201394, 0.256904  , 0.303902  , 0.21718007], dtype=float32)],\n 28: [array([0.516433  , 0.22613506, 0.2250492 , 0.03238266], dtype=float32),\n  array([0.5271467 , 0.22257294, 0.22949411, 0.0207862 ], dtype=float32),\n  array([0.03508924, 0.20863305, 0.33798203, 0.41829568], dtype=float32),\n  array([0.03391634, 0.22095633, 0.34386355, 0.40126377], dtype=float32)],\n 29: [array([0.8337356 , 0.10359976, 0.04403352, 0.01863117], dtype=float32),\n  array([0.7702869 , 0.1310083 , 0.06853897, 0.03016573], dtype=float32),\n  array([0.05780712, 0.27242965, 0.34664226, 0.323121  ], dtype=float32),\n  array([0.03587726, 0.29287764, 0.3581711 , 0.31307402], dtype=float32)],\n 30: [array([0.5780819 , 0.30755794, 0.07317305, 0.04118716], dtype=float32),\n  array([0.537238  , 0.3162223 , 0.09365035, 0.0528893 ], dtype=float32),\n  array([0.063335  , 0.23878516, 0.36790955, 0.32997027], dtype=float32),\n  array([0.02994946, 0.25394785, 0.40036762, 0.31573504], dtype=float32)],\n 31: [array([0.52727896, 0.18274511, 0.18707757, 0.10289837], dtype=float32),\n  array([0.49554455, 0.19562371, 0.20045723, 0.10837454], dtype=float32),\n  array([0.02136632, 0.19650215, 0.47255808, 0.30957344], dtype=float32),\n  array([0.01099438, 0.20093487, 0.4969729 , 0.29109782], dtype=float32)],\n 32: [array([0.56285185, 0.2994812 , 0.1113913 , 0.0262756 ], dtype=float32),\n  array([0.56427324, 0.2888905 , 0.11987507, 0.02696115], dtype=float32),\n  array([0.07870439, 0.27609465, 0.45645204, 0.18874888], dtype=float32),\n  array([0.04330696, 0.29182515, 0.50354636, 0.16132154], dtype=float32)],\n 33: [array([0.5434278 , 0.33843952, 0.10207145, 0.01606124], dtype=float32),\n  array([0.49735576, 0.35857606, 0.12394947, 0.02011874], dtype=float32),\n  array([0.01690718, 0.13025174, 0.50551367, 0.34732744], dtype=float32),\n  array([0.01347684, 0.13277236, 0.51065356, 0.3430972 ], dtype=float32)],\n 34: [array([0.64768034, 0.33081338, 0.01850327, 0.00300298], dtype=float32),\n  array([0.6188738 , 0.36373657, 0.01545355, 0.00193617], dtype=float32),\n  array([0.01895958, 0.16752605, 0.43983224, 0.37368214], dtype=float32),\n  array([0.04825506, 0.23454109, 0.39762053, 0.3195834 ], dtype=float32)],\n 35: [array([0.28256214, 0.42799664, 0.2515646 , 0.03787673], dtype=float32),\n  array([0.22942442, 0.50157255, 0.24854495, 0.02045803], dtype=float32),\n  array([0.13981922, 0.40272322, 0.31422946, 0.14322811], dtype=float32),\n  array([0.12688746, 0.5309733 , 0.2797271 , 0.06241218], dtype=float32)],\n 36: [array([0.48820853, 0.23686452, 0.20668216, 0.06824481], dtype=float32),\n  array([0.52109325, 0.18369377, 0.22298484, 0.07222804], dtype=float32),\n  array([0.041363 , 0.253465 , 0.3729016, 0.3322703], dtype=float32),\n  array([0.06560296, 0.2943422 , 0.34430823, 0.29574662], dtype=float32)],\n 37: [array([0.61013377, 0.14305568, 0.15276219, 0.09404833], dtype=float32),\n  array([0.56235707, 0.10901016, 0.19506374, 0.13356906], dtype=float32),\n  array([0.02108127, 0.23716053, 0.501831  , 0.23992726], dtype=float32),\n  array([0.05611274, 0.29081726, 0.41931275, 0.23375721], dtype=float32)],\n 38: [array([0.48646915, 0.29203668, 0.17940088, 0.04209329], dtype=float32),\n  array([0.48392525, 0.22909637, 0.22419478, 0.06278358], dtype=float32),\n  array([0.14410639, 0.24945205, 0.43034393, 0.17609766], dtype=float32),\n  array([0.17878866, 0.2818364 , 0.3843268 , 0.15504815], dtype=float32)],\n 39: [array([0.65405613, 0.17852259, 0.07505376, 0.09236755], dtype=float32),\n  array([0.6905194 , 0.13597286, 0.07742957, 0.09607825], dtype=float32),\n  array([0.10594556, 0.22785263, 0.37067008, 0.29553166], dtype=float32),\n  array([0.34598207, 0.22707699, 0.21675977, 0.21018115], dtype=float32)],\n 40: [array([0.5281449 , 0.1556013 , 0.12676837, 0.18948539], dtype=float32),\n  array([0.5132865, 0.1384075, 0.1446627, 0.2036434], dtype=float32),\n  array([0.00394942, 0.05520783, 0.25218084, 0.688662  ], dtype=float32),\n  array([0.01978046, 0.10396949, 0.28580335, 0.5904467 ], dtype=float32)],\n 41: [array([0.52655697, 0.24850193, 0.11640982, 0.1085313 ], dtype=float32),\n  array([0.54271173, 0.21698098, 0.11746153, 0.12284583], dtype=float32),\n  array([0.0825073 , 0.33594808, 0.26230714, 0.31923753], dtype=float32),\n  array([0.322271  , 0.28463042, 0.18671086, 0.20638776], dtype=float32)],\n 42: [array([0.6951756 , 0.1977271 , 0.07088583, 0.03621146], dtype=float32),\n  array([0.68391097, 0.19065525, 0.07816007, 0.04727364], dtype=float32),\n  array([0.19870958, 0.27541918, 0.26709297, 0.25877824], dtype=float32),\n  array([0.18830499, 0.25974706, 0.2732065 , 0.27874145], dtype=float32)],\n 43: [array([0.38774166, 0.4065832 , 0.14065355, 0.06502164], dtype=float32),\n  array([0.3850003 , 0.37317386, 0.15697949, 0.08484641], dtype=float32),\n  array([0.08516864, 0.2803891 , 0.28603858, 0.34840366], dtype=float32),\n  array([0.10610053, 0.28275132, 0.27396238, 0.33718574], dtype=float32)],\n 44: [array([0.43660712, 0.36255634, 0.17311563, 0.02772087], dtype=float32),\n  array([0.41234013, 0.35110343, 0.19212069, 0.04443582], dtype=float32),\n  array([0.03011516, 0.34021667, 0.2954855 , 0.33418265], dtype=float32),\n  array([0.04158999, 0.33403084, 0.2660655 , 0.35831377], dtype=float32)],\n 45: [array([0.61482894, 0.16762626, 0.1482402 , 0.06930459], dtype=float32),\n  array([0.5168674 , 0.19659606, 0.18214574, 0.10439077], dtype=float32),\n  array([0.0201121 , 0.27709574, 0.21614183, 0.48665035], dtype=float32),\n  array([0.04487789, 0.2989884 , 0.2136294 , 0.44250435], dtype=float32)],\n 46: [array([0.51408297, 0.37610295, 0.08839662, 0.0214175 ], dtype=float32),\n  array([0.48847765, 0.4127929 , 0.08099223, 0.01773728], dtype=float32),\n  array([0.00993452, 0.23611878, 0.39098376, 0.362963  ], dtype=float32),\n  array([0.02550009, 0.26171482, 0.34158048, 0.3712046 ], dtype=float32)],\n 47: [array([0.4804346 , 0.2866709 , 0.17333077, 0.05956373], dtype=float32),\n  array([0.4922486 , 0.32211512, 0.15205795, 0.03357836], dtype=float32),\n  array([0.00220482, 0.18875833, 0.3452477 , 0.4637892 ], dtype=float32),\n  array([0.0177249 , 0.3110448 , 0.32724026, 0.3439901 ], dtype=float32)],\n 48: [array([0.6545059 , 0.15373527, 0.14230418, 0.04945461], dtype=float32),\n  array([0.62887114, 0.16361268, 0.1555455 , 0.05197068], dtype=float32),\n  array([0.02905987, 0.17869024, 0.32713658, 0.4651133 ], dtype=float32),\n  array([0.06955867, 0.26537183, 0.31114334, 0.3539262 ], dtype=float32)],\n 49: [array([0.55688775, 0.2245758 , 0.13200557, 0.08653098], dtype=float32),\n  array([0.75099194, 0.16687794, 0.0569748 , 0.02515527], dtype=float32),\n  array([0.01031758, 0.10359165, 0.48523962, 0.40085122], dtype=float32),\n  array([0.05361047, 0.22113745, 0.3397495 , 0.38550258], dtype=float32)],\n 50: [array([0.3620397 , 0.3841381 , 0.19479822, 0.05902402], dtype=float32),\n  array([0.34335658, 0.45905116, 0.17352092, 0.02407146], dtype=float32),\n  array([0.10649144, 0.28541073, 0.29675055, 0.31134725], dtype=float32),\n  array([0.08899764, 0.32310224, 0.26479402, 0.3231061 ], dtype=float32)],\n 51: [array([0.8029858 , 0.1356781 , 0.05201279, 0.00932331], dtype=float32),\n  array([0.89610356, 0.08270395, 0.01987453, 0.00131797], dtype=float32),\n  array([0.1481332 , 0.2477131 , 0.33727732, 0.26687637], dtype=float32),\n  array([0.1633545 , 0.2616287 , 0.2913851 , 0.28363165], dtype=float32)],\n 52: [array([0.4782418 , 0.21666837, 0.12623776, 0.17885204], dtype=float32),\n  array([0.5541187 , 0.21685088, 0.10799568, 0.1210348 ], dtype=float32),\n  array([0.06018409, 0.28217933, 0.22796945, 0.42966715], dtype=float32),\n  array([0.04546843, 0.28913882, 0.21973819, 0.44565448], dtype=float32)],\n 53: [array([0.60881746, 0.17876245, 0.09662829, 0.11579175], dtype=float32),\n  array([0.7474085 , 0.15310575, 0.05775104, 0.0417348 ], dtype=float32),\n  array([0.05847624, 0.1906157 , 0.2598287 , 0.49107936], dtype=float32),\n  array([0.07463994, 0.20620295, 0.23776828, 0.48138884], dtype=float32)],\n 54: [array([0.52931416, 0.18707912, 0.13464497, 0.14896172], dtype=float32),\n  array([0.5716631 , 0.19172458, 0.12664954, 0.10996278], dtype=float32),\n  array([0.01619231, 0.13516338, 0.3547245 , 0.49391982], dtype=float32),\n  array([0.02260281, 0.15365547, 0.34101793, 0.4827238 ], dtype=float32)],\n 55: [array([0.6816562 , 0.13386747, 0.14120811, 0.04326821], dtype=float32),\n  array([0.69553053, 0.13055298, 0.14363441, 0.0302821 ], dtype=float32),\n  array([0.16950716, 0.18109997, 0.42198905, 0.22740385], dtype=float32),\n  array([0.14301495, 0.18087575, 0.41894013, 0.2571692 ], dtype=float32)],\n 56: [array([0.5851984 , 0.18282728, 0.16538507, 0.06658923], dtype=float32),\n  array([0.5962998 , 0.18513805, 0.16561046, 0.05295158], dtype=float32),\n  array([0.07362564, 0.10055903, 0.5871184 , 0.23869698], dtype=float32),\n  array([0.1033705 , 0.11763259, 0.5194786 , 0.2595183 ], dtype=float32)],\n 57: [array([0.5384505 , 0.24221784, 0.13128519, 0.08804649], dtype=float32),\n  array([0.55936456, 0.2606266 , 0.11840852, 0.06160029], dtype=float32),\n  array([0.03683178, 0.13015157, 0.35656244, 0.47645426], dtype=float32),\n  array([0.05472375, 0.14563283, 0.34119007, 0.45845336], dtype=float32)],\n 58: [array([0.89462024, 0.08222549, 0.0178939 , 0.00526038], dtype=float32),\n  array([9.5398438e-01, 4.0873103e-02, 4.4830767e-03, 6.5940019e-04],\n        dtype=float32),\n  array([0.20265378, 0.26582876, 0.3027045 , 0.22881295], dtype=float32),\n  array([0.28033894, 0.27283257, 0.257878  , 0.18895055], dtype=float32)],\n 59: [array([0.6157517 , 0.16084917, 0.14460857, 0.07879058], dtype=float32),\n  array([0.62750995, 0.16543233, 0.14865081, 0.05840689], dtype=float32),\n  array([0.03877728, 0.14478937, 0.3070416 , 0.5093918 ], dtype=float32),\n  array([0.10376254, 0.18560801, 0.28507283, 0.42555663], dtype=float32)],\n 60: [array([0.5875389 , 0.15286615, 0.16936857, 0.09022633], dtype=float32),\n  array([0.74857956, 0.11384888, 0.11510057, 0.02247105], dtype=float32),\n  array([0.00699802, 0.09682771, 0.32120532, 0.57496893], dtype=float32),\n  array([0.03221799, 0.14774854, 0.32174113, 0.49829233], dtype=float32)],\n 61: [array([0.75532323, 0.12454335, 0.09508851, 0.02504489], dtype=float32),\n  array([0.84315056, 0.09121503, 0.05931459, 0.00631977], dtype=float32),\n  array([0.05791165, 0.14940766, 0.31876376, 0.47391698], dtype=float32),\n  array([0.07102916, 0.16457397, 0.3038873 , 0.4605096 ], dtype=float32)],\n 62: [array([0.6302664 , 0.2696684 , 0.09580325, 0.00426195], dtype=float32),\n  array([0.63586813, 0.28600603, 0.07663568, 0.0014901 ], dtype=float32),\n  array([0.25165078, 0.2562003 , 0.3914625 , 0.10068639], dtype=float32),\n  array([0.21308249, 0.28907537, 0.37151706, 0.12632509], dtype=float32)],\n 63: [array([0.5757351 , 0.18857878, 0.12840033, 0.10728579], dtype=float32),\n  array([0.69983405, 0.1880228 , 0.08083475, 0.03130838], dtype=float32),\n  array([0.21142277, 0.18125387, 0.27986303, 0.32746026], dtype=float32),\n  array([0.20551105, 0.19639671, 0.27968794, 0.3184043 ], dtype=float32)],\n 64: [array([0.53263557, 0.28136632, 0.12789467, 0.05810345], dtype=float32),\n  array([0.57919   , 0.36206564, 0.05282743, 0.00591691], dtype=float32),\n  array([0.22787751, 0.232006  , 0.2892027 , 0.2509138 ], dtype=float32),\n  array([0.19542319, 0.26507103, 0.30640122, 0.23310451], dtype=float32)],\n 65: [array([0.51341945, 0.26105684, 0.19655184, 0.02897194], dtype=float32),\n  array([0.5442192 , 0.32221895, 0.13160406, 0.00195781], dtype=float32),\n  array([0.11298833, 0.28409153, 0.46596357, 0.13695653], dtype=float32),\n  array([0.10073593, 0.29255196, 0.48209482, 0.12461731], dtype=float32)],\n 66: [array([0.4868591 , 0.1814422 , 0.17384872, 0.15785   ], dtype=float32),\n  array([0.6067559 , 0.20734814, 0.1295069 , 0.05638907], dtype=float32),\n  array([0.08495581, 0.15738674, 0.27795854, 0.4796989 ], dtype=float32),\n  array([0.09877101, 0.17039174, 0.2734048 , 0.45743242], dtype=float32)],\n 67: [array([0.4929636 , 0.24916132, 0.1377199 , 0.12015527], dtype=float32),\n  array([0.58195263, 0.30113995, 0.07408804, 0.0428194 ], dtype=float32),\n  array([0.03453329, 0.20484951, 0.3166363 , 0.4439809 ], dtype=float32),\n  array([0.05145582, 0.23192468, 0.3072705 , 0.40934896], dtype=float32)],\n 68: [array([0.4844556 , 0.178459  , 0.16593084, 0.17115465], dtype=float32),\n  array([0.58174354, 0.18617478, 0.1268355 , 0.10524616], dtype=float32),\n  array([0.05395971, 0.16543722, 0.4211974 , 0.35940564], dtype=float32),\n  array([0.06753973, 0.18209416, 0.39847282, 0.35189337], dtype=float32)],\n 69: [array([0.7765178 , 0.1437592 , 0.05316956, 0.02655337], dtype=float32),\n  array([0.84394497, 0.1222622 , 0.02559658, 0.00819619], dtype=float32),\n  array([0.3170779 , 0.2859525 , 0.22887534, 0.1680942 ], dtype=float32),\n  array([0.24476443, 0.31968516, 0.25816795, 0.17738238], dtype=float32)],\n 70: [array([0.55687773, 0.17743327, 0.14703497, 0.118654  ], dtype=float32),\n  array([0.5752744 , 0.18589191, 0.14116544, 0.09766828], dtype=float32),\n  array([0.05384161, 0.2191042 , 0.43374243, 0.29331186], dtype=float32),\n  array([0.05289082, 0.22246519, 0.43405753, 0.29058644], dtype=float32)],\n 71: [array([0.5005568 , 0.15611237, 0.15997458, 0.1833562 ], dtype=float32),\n  array([0.61657673, 0.14874518, 0.12131185, 0.11336615], dtype=float32),\n  array([0.02245965, 0.15574388, 0.47456595, 0.34723058], dtype=float32),\n  array([0.02357084, 0.15886329, 0.46949562, 0.3480702 ], dtype=float32)],\n 72: [array([0.60022056, 0.12512158, 0.13519901, 0.13945878], dtype=float32),\n  array([0.7755089 , 0.09377829, 0.06914832, 0.06156451], dtype=float32),\n  array([0.0383176 , 0.09809257, 0.4685504 , 0.39503947], dtype=float32),\n  array([0.04430293, 0.10384385, 0.46152285, 0.39033028], dtype=float32)],\n 73: [array([0.5111721 , 0.09595595, 0.24039064, 0.15248138], dtype=float32),\n  array([0.5376816 , 0.09121468, 0.24252412, 0.12857968], dtype=float32),\n  array([0.00931398, 0.04403548, 0.43579993, 0.51085067], dtype=float32),\n  array([0.01131747, 0.04792709, 0.4429765 , 0.49777904], dtype=float32)],\n 74: [array([0.58321935, 0.09747544, 0.1763087 , 0.14299645], dtype=float32),\n  array([0.63732   , 0.09388188, 0.15965503, 0.10914311], dtype=float32),\n  array([0.02905486, 0.07282253, 0.37910065, 0.5190219 ], dtype=float32),\n  array([0.0292735 , 0.07416435, 0.38161469, 0.5149474 ], dtype=float32)],\n 75: [array([0.58340096, 0.17953292, 0.1611445 , 0.0759217 ], dtype=float32),\n  array([0.5752335 , 0.20266238, 0.1517068 , 0.0703973 ], dtype=float32),\n  array([0.15675603, 0.17061374, 0.45097774, 0.22165251], dtype=float32),\n  array([0.15963496, 0.17710124, 0.446228  , 0.21703577], dtype=float32)],\n 76: [array([0.46686116, 0.18245181, 0.20157619, 0.1491109 ], dtype=float32),\n  array([0.45847607, 0.2097305 , 0.1829036 , 0.14888984], dtype=float32),\n  array([0.03994572, 0.09804387, 0.36994046, 0.49206996], dtype=float32),\n  array([0.04325302, 0.10206102, 0.36854532, 0.4861406 ], dtype=float32)],\n 77: [array([0.73986554, 0.13000102, 0.08630246, 0.04383091], dtype=float32),\n  array([0.69118166, 0.16093671, 0.09076864, 0.05711306], dtype=float32),\n  array([0.10612909, 0.20538054, 0.3889987 , 0.2994917 ], dtype=float32),\n  array([0.08205515, 0.20225683, 0.39686602, 0.31882197], dtype=float32)],\n 78: [array([0.70744187, 0.10612158, 0.1037574 , 0.08267921], dtype=float32),\n  array([0.62562317, 0.13184668, 0.12011657, 0.12241349], dtype=float32),\n  array([0.02643071, 0.18948178, 0.31706798, 0.46701962], dtype=float32),\n  array([0.02888549, 0.1962176 , 0.3135147 , 0.46138218], dtype=float32)],\n 79: [array([0.52585536, 0.13413419, 0.24660034, 0.09341013], dtype=float32),\n  array([0.5296312 , 0.14542009, 0.22445233, 0.10049634], dtype=float32),\n  array([0.11262275, 0.19938432, 0.4070719 , 0.28092098], dtype=float32),\n  array([0.13435623, 0.20278738, 0.39704958, 0.26580676], dtype=float32)],\n 80: [array([0.5539926 , 0.11887252, 0.20306535, 0.12406944], dtype=float32),\n  array([0.5244172 , 0.13023673, 0.19614027, 0.14920577], dtype=float32),\n  array([0.02299835, 0.10965095, 0.4601874 , 0.4071633 ], dtype=float32),\n  array([0.0450011 , 0.12892021, 0.43534648, 0.3907321 ], dtype=float32)],\n 81: [array([0.8373162 , 0.11695488, 0.03701866, 0.00871021], dtype=float32),\n  array([0.8099328 , 0.14021926, 0.03748683, 0.01236111], dtype=float32),\n  array([0.1405504 , 0.30254546, 0.40247625, 0.15442789], dtype=float32),\n  array([0.1496759 , 0.29299662, 0.3886784 , 0.1686491 ], dtype=float32)],\n 82: [array([0.879001  , 0.10065026, 0.01368695, 0.00666186], dtype=float32),\n  array([0.8944914 , 0.09373266, 0.00742967, 0.00434634], dtype=float32),\n  array([0.06804588, 0.19544202, 0.37939045, 0.35712168], dtype=float32),\n  array([0.10332785, 0.22313789, 0.3519171 , 0.3216172 ], dtype=float32)],\n 83: [array([0.54712266, 0.22726344, 0.1426133 , 0.08300056], dtype=float32),\n  array([0.5474874 , 0.2436855 , 0.12978779, 0.07903932], dtype=float32),\n  array([0.01607786, 0.26400188, 0.56310946, 0.1568108 ], dtype=float32),\n  array([0.02375058, 0.2788377 , 0.5371089 , 0.16030276], dtype=float32)],\n 84: [array([0.7768507 , 0.1527017 , 0.05516731, 0.01528027], dtype=float32),\n  array([0.74721515, 0.17317933, 0.06068762, 0.0189179 ], dtype=float32),\n  array([0.06368721, 0.2652751 , 0.55144244, 0.11959523], dtype=float32),\n  array([0.08673651, 0.28233454, 0.5124406 , 0.11848834], dtype=float32)],\n 85: [array([0.7378443 , 0.16532806, 0.07161096, 0.02521676], dtype=float32),\n  array([0.7322386 , 0.17308259, 0.07039566, 0.02428311], dtype=float32),\n  array([0.04725593, 0.4283858 , 0.30604395, 0.2183143 ], dtype=float32),\n  array([0.06750146, 0.42749813, 0.29849905, 0.2065014 ], dtype=float32)],\n 86: [array([0.68341094, 0.10890912, 0.14567006, 0.06200989], dtype=float32),\n  array([0.63787323, 0.123596  , 0.1649594 , 0.07357131], dtype=float32),\n  array([0.08495606, 0.24681227, 0.26846957, 0.39976215], dtype=float32),\n  array([0.10225992, 0.24411592, 0.2698625 , 0.38376167], dtype=float32)],\n 87: [array([0.5005523 , 0.17229846, 0.27135342, 0.05579579], dtype=float32),\n  array([0.5233106 , 0.16171135, 0.28708023, 0.02789792], dtype=float32),\n  array([0.10688116, 0.22302149, 0.2367116 , 0.43338567], dtype=float32),\n  array([0.13241059, 0.22179149, 0.24158831, 0.4042096 ], dtype=float32)],\n 88: [array([0.42831817, 0.15043148, 0.2285705 , 0.19267976], dtype=float32),\n  array([0.691901  , 0.14318447, 0.13802192, 0.02689262], dtype=float32),\n  array([0.0659377 , 0.18325737, 0.32576147, 0.42504352], dtype=float32),\n  array([0.15302451, 0.1925079 , 0.3113271 , 0.34314045], dtype=float32)],\n 89: [array([0.4977235 , 0.21429649, 0.17067522, 0.11730476], dtype=float32),\n  array([0.6265203 , 0.27719402, 0.06537506, 0.03091061], dtype=float32),\n  array([0.0681586 , 0.28359038, 0.34495392, 0.30329704], dtype=float32),\n  array([0.10817066, 0.28495896, 0.32688966, 0.27998072], dtype=float32)],\n 90: [array([0.5534411 , 0.20712005, 0.1466285 , 0.09281036], dtype=float32),\n  array([0.5926816 , 0.21901913, 0.12102762, 0.06727167], dtype=float32),\n  array([0.10123289, 0.22036785, 0.3210308 , 0.3573685 ], dtype=float32),\n  array([0.16035868, 0.22626953, 0.30173776, 0.311634  ], dtype=float32)],\n 91: [array([0.58802134, 0.2596356 , 0.08829432, 0.06404881], dtype=float32),\n  array([0.59376496, 0.27467176, 0.07365026, 0.05791299], dtype=float32),\n  array([0.06869893, 0.30390316, 0.2945641 , 0.33283377], dtype=float32),\n  array([0.13572341, 0.2976923 , 0.27115005, 0.2954342 ], dtype=float32)],\n 92: [array([0.71380234, 0.11404793, 0.10537066, 0.0667791 ], dtype=float32),\n  array([0.6146859 , 0.13793418, 0.13450937, 0.1128706 ], dtype=float32),\n  array([0.07947721, 0.18251185, 0.40845042, 0.32956046], dtype=float32),\n  array([0.13164915, 0.18471588, 0.37061733, 0.3130176 ], dtype=float32)],\n 93: [array([0.54573816, 0.13349646, 0.19682983, 0.12393559], dtype=float32),\n  array([0.5395698 , 0.14335029, 0.18205151, 0.13502835], dtype=float32),\n  array([0.03216616, 0.16137724, 0.4679425 , 0.33851406], dtype=float32),\n  array([0.10754429, 0.17832874, 0.39484227, 0.3192847 ], dtype=float32)],\n 94: [array([0.7471808 , 0.10959271, 0.11632738, 0.02689909], dtype=float32),\n  array([0.7590125 , 0.12136073, 0.09366421, 0.02596257], dtype=float32),\n  array([0.08851478, 0.25943625, 0.38956386, 0.26248512], dtype=float32),\n  array([0.20195821, 0.2438961 , 0.33709893, 0.21704672], dtype=float32)],\n 95: [array([0.6043592 , 0.06555592, 0.26609913, 0.06398573], dtype=float32),\n  array([0.686345  , 0.05830667, 0.21783134, 0.03751695], dtype=float32),\n  array([0.01919853, 0.15545624, 0.41554472, 0.40980044], dtype=float32),\n  array([0.06200348, 0.17280711, 0.39093098, 0.37425846], dtype=float32)],\n 96: [array([0.5531328 , 0.05665435, 0.3333881 , 0.05682477], dtype=float32),\n  array([0.54407173, 0.08004143, 0.30004397, 0.07584289], dtype=float32),\n  array([0.03495332, 0.15884155, 0.34275874, 0.4634464 ], dtype=float32),\n  array([0.04638294, 0.16335313, 0.34211716, 0.44814676], dtype=float32)],\n 97: [array([0.8137875 , 0.0806377 , 0.09666722, 0.00890754], dtype=float32),\n  array([0.6661068 , 0.1557129 , 0.14654785, 0.03163254], dtype=float32),\n  array([0.04353257, 0.23748927, 0.5454664 , 0.17351179], dtype=float32),\n  array([0.02834338, 0.25396854, 0.54854196, 0.16914617], dtype=float32)],\n 98: [array([0.56993234, 0.18490086, 0.19814894, 0.04701785], dtype=float32),\n  array([0.52230304, 0.22591129, 0.19600616, 0.05577952], dtype=float32),\n  array([0.38039488, 0.20247906, 0.2744324 , 0.14269356], dtype=float32),\n  array([0.2387435 , 0.27728042, 0.3381667 , 0.1458094 ], dtype=float32)],\n 99: [array([0.53182155, 0.1608789 , 0.19257714, 0.11472243], dtype=float32),\n  array([0.5534802 , 0.18082984, 0.16927521, 0.09641478], dtype=float32),\n  array([0.04771253, 0.17497426, 0.44002423, 0.33728895], dtype=float32),\n  array([0.07099754, 0.18348071, 0.4184232 , 0.32709855], dtype=float32)],\n 100: [array([0.7234294 , 0.16049561, 0.08649054, 0.02958448], dtype=float32),\n  array([0.71962804, 0.1707536 , 0.08265699, 0.02696134], dtype=float32),\n  array([0.05281997, 0.20494859, 0.26121205, 0.4810194 ], dtype=float32),\n  array([0.173648  , 0.23779806, 0.25545928, 0.33309463], dtype=float32)],\n 101: [array([0.59687513, 0.198158  , 0.13776009, 0.06720677], dtype=float32),\n  array([0.6060707 , 0.21241981, 0.13089366, 0.05061578], dtype=float32),\n  array([0.01157905, 0.13755448, 0.31943873, 0.53142774], dtype=float32),\n  array([0.03279502, 0.16451587, 0.32659912, 0.47608995], dtype=float32)],\n 102: [array([0.66391426, 0.16579396, 0.14572693, 0.02456478], dtype=float32),\n  array([0.6613681 , 0.1737285 , 0.14603807, 0.01886534], dtype=float32),\n  array([0.07288994, 0.20662874, 0.31662118, 0.40386018], dtype=float32),\n  array([0.11599914, 0.22662483, 0.32035428, 0.33702174], dtype=float32)],\n 103: [array([0.5576476 , 0.26428366, 0.14142561, 0.03664318], dtype=float32),\n  array([0.5511177 , 0.281214  , 0.14328763, 0.02438061], dtype=float32),\n  array([0.07368828, 0.276741  , 0.34125417, 0.30831653], dtype=float32),\n  array([0.09390047, 0.31848916, 0.34076902, 0.2468413 ], dtype=float32)],\n 104: [array([0.66541374, 0.29117906, 0.03598527, 0.00742198], dtype=float32),\n  array([0.6398921 , 0.33426106, 0.02334273, 0.00250409], dtype=float32),\n  array([0.05858401, 0.3185375 , 0.4106345 , 0.21224399], dtype=float32),\n  array([0.05757037, 0.35403028, 0.3948756 , 0.19352381], dtype=float32)],\n 105: [array([0.698725  , 0.21688963, 0.05557252, 0.02881283], dtype=float32),\n  array([0.6158669 , 0.23903212, 0.09613788, 0.04896308], dtype=float32),\n  array([0.0166774 , 0.2518643 , 0.47687116, 0.25458714], dtype=float32),\n  array([0.01980463, 0.26324472, 0.45946884, 0.25748187], dtype=float32)],\n 106: [array([0.7104074 , 0.23306331, 0.03078792, 0.02574141], dtype=float32),\n  array([0.6080683 , 0.23469138, 0.08557086, 0.07166941], dtype=float32),\n  array([0.05581239, 0.2881553 , 0.3188488 , 0.3371835 ], dtype=float32),\n  array([0.09303218, 0.305019  , 0.29039168, 0.31155717], dtype=float32)],\n 107: [array([0.7113057 , 0.20185508, 0.06455673, 0.0222825 ], dtype=float32),\n  array([0.6192332 , 0.23197033, 0.10457259, 0.04422389], dtype=float32),\n  array([0.01582891, 0.30211988, 0.3367768 , 0.34527442], dtype=float32),\n  array([0.03182549, 0.28839493, 0.3277375 , 0.35204202], dtype=float32)],\n 108: [array([0.7646618 , 0.1182673 , 0.10852855, 0.00854237], dtype=float32),\n  array([0.7062853 , 0.14401798, 0.13559557, 0.0141011 ], dtype=float32),\n  array([0.01215085, 0.48221365, 0.37912187, 0.12651359], dtype=float32),\n  array([0.03626487, 0.4232142 , 0.37468103, 0.16583988], dtype=float32)],\n 109: [array([0.81169665, 0.09707475, 0.05681083, 0.03441788], dtype=float32),\n  array([0.7299375 , 0.13065661, 0.08756167, 0.05184415], dtype=float32),\n  array([0.1103203 , 0.39468354, 0.29621065, 0.19878551], dtype=float32),\n  array([0.14027911, 0.35001132, 0.29163554, 0.21807407], dtype=float32)],\n 110: [array([0.6573182 , 0.09126984, 0.1327699 , 0.11864199], dtype=float32),\n  array([0.6211131 , 0.11374773, 0.15318882, 0.11195034], dtype=float32),\n  array([0.02062383, 0.17611583, 0.2293903 , 0.57387006], dtype=float32),\n  array([0.03668907, 0.18106923, 0.24448735, 0.53775436], dtype=float32)],\n 111: [array([0.67911714, 0.0830726 , 0.12867595, 0.10913436], dtype=float32),\n  array([0.57078105, 0.12024012, 0.16563891, 0.14333995], dtype=float32),\n  array([0.00431848, 0.10477309, 0.10286915, 0.78803927], dtype=float32),\n  array([0.01433179, 0.12944138, 0.14267904, 0.7135478 ], dtype=float32)],\n 112: [array([0.6285112 , 0.10897917, 0.14714184, 0.11536783], dtype=float32),\n  array([0.7728438 , 0.08848736, 0.0907416 , 0.0479273 ], dtype=float32),\n  array([0.0868849 , 0.2074945 , 0.33462402, 0.3709965 ], dtype=float32),\n  array([0.17317598, 0.20612383, 0.3124541 , 0.30824605], dtype=float32)],\n 113: [array([0.6866684 , 0.18823233, 0.09920963, 0.02588969], dtype=float32),\n  array([0.71956116, 0.191336  , 0.07696979, 0.01213303], dtype=float32),\n  array([0.10022639, 0.32348138, 0.40961507, 0.16667713], dtype=float32),\n  array([0.07516909, 0.32943064, 0.42730382, 0.16809645], dtype=float32)],\n 114: [array([0.8103946 , 0.08476226, 0.0954222 , 0.00942105], dtype=float32),\n  array([0.8130067 , 0.08154713, 0.0984335 , 0.00701272], dtype=float32),\n  array([0.05328201, 0.26278263, 0.53580076, 0.14813459], dtype=float32),\n  array([0.07615592, 0.26306397, 0.5118522 , 0.14892794], dtype=float32)],\n 115: [array([0.5117632 , 0.1505504 , 0.19090903, 0.14677736], dtype=float32),\n  array([0.59222555, 0.15033221, 0.18287523, 0.07456703], dtype=float32),\n  array([0.01486374, 0.1520352 , 0.31492788, 0.51817316], dtype=float32),\n  array([0.02564211, 0.162462  , 0.32583195, 0.48606393], dtype=float32)],\n 116: [array([0.5084494 , 0.19629548, 0.15489286, 0.14036229], dtype=float32),\n  array([0.5685791 , 0.17295924, 0.16820066, 0.09026093], dtype=float32),\n  array([0.01066352, 0.27085248, 0.26521984, 0.4532642 ], dtype=float32),\n  array([0.01714353, 0.26427743, 0.27313653, 0.44544256], dtype=float32)],\n 117: [array([0.5680654 , 0.22375068, 0.14983141, 0.05835248], dtype=float32),\n  array([0.652332  , 0.15231559, 0.16878612, 0.02656631], dtype=float32),\n  array([0.06604322, 0.21488501, 0.35759467, 0.36147705], dtype=float32),\n  array([0.14298223, 0.21840039, 0.3522901 , 0.28632727], dtype=float32)],\n 118: [array([0.4973054 , 0.2261523 , 0.10137583, 0.17516655], dtype=float32),\n  array([0.635582  , 0.2653679 , 0.05796894, 0.04108117], dtype=float32),\n  array([0.07812235, 0.15502499, 0.29201007, 0.47484264], dtype=float32),\n  array([0.08478669, 0.15690467, 0.29195118, 0.46635753], dtype=float32)],\n 119: [array([0.50064963, 0.36594906, 0.06977033, 0.06363104], dtype=float32),\n  array([0.5534874 , 0.42052263, 0.02097546, 0.00501453], dtype=float32),\n  array([0.02626048, 0.12221397, 0.4279058 , 0.42361975], dtype=float32),\n  array([0.03129701, 0.1286442 , 0.42689583, 0.41316295], dtype=float32)],\n 120: [array([0.6142203 , 0.24467638, 0.09723375, 0.0438695 ], dtype=float32),\n  array([0.6850853 , 0.21643053, 0.08851961, 0.00996452], dtype=float32),\n  array([0.09361248, 0.17217167, 0.31853798, 0.41567788], dtype=float32),\n  array([0.09285481, 0.17192   , 0.31869808, 0.41652703], dtype=float32)],\n 121: [array([0.48788667, 0.22545534, 0.20250121, 0.08415681], dtype=float32),\n  array([0.4944765 , 0.19981897, 0.24552517, 0.06017932], dtype=float32),\n  array([0.03720588, 0.1988027 , 0.3135431 , 0.45044836], dtype=float32),\n  array([0.03659425, 0.19845119, 0.3135816 , 0.45137304], dtype=float32)],\n 122: [array([0.5882835 , 0.14473931, 0.16385344, 0.10312381], dtype=float32),\n  array([0.5175606 , 0.13210087, 0.19762595, 0.15271252], dtype=float32),\n  array([0.04629188, 0.09823246, 0.27907813, 0.57639754], dtype=float32),\n  array([0.04503419, 0.09753116, 0.2785344 , 0.5789002 ], dtype=float32)],\n 123: [array([0.6566558 , 0.14798228, 0.17446215, 0.02089979], dtype=float32),\n  array([0.6453328 , 0.14438061, 0.18477377, 0.02551286], dtype=float32),\n  array([0.10005391, 0.19206594, 0.4406478 , 0.2672324 ], dtype=float32),\n  array([0.09616743, 0.19074127, 0.44164914, 0.27144217], dtype=float32)],\n 124: [array([0.7392077 , 0.1737113 , 0.07825869, 0.00882231], dtype=float32),\n  array([0.7568175 , 0.16110912, 0.07513343, 0.00693986], dtype=float32),\n  array([0.03580013, 0.22806266, 0.35434967, 0.38178754], dtype=float32),\n  array([0.03522079, 0.22735353, 0.35446438, 0.3829613 ], dtype=float32)],\n 125: [array([0.40002698, 0.4072635 , 0.15269886, 0.04001065], dtype=float32),\n  array([0.4292879 , 0.31590173, 0.18102898, 0.0737813 ], dtype=float32),\n  array([0.07514853, 0.27907783, 0.4198964 , 0.22587723], dtype=float32),\n  array([0.07365613, 0.27844152, 0.42135116, 0.2265512 ], dtype=float32)],\n 126: [array([0.69294393, 0.17851576, 0.1112424 , 0.01729785], dtype=float32),\n  array([0.65619874, 0.18278669, 0.13065554, 0.03035905], dtype=float32),\n  array([0.16187061, 0.2855376 , 0.36190626, 0.19068547], dtype=float32),\n  array([0.16053052, 0.28594533, 0.3626226 , 0.19090149], dtype=float32)],\n 127: [array([0.78005624, 0.12327007, 0.06016863, 0.0365051 ], dtype=float32),\n  array([0.77946544, 0.12905556, 0.05807302, 0.03340597], dtype=float32),\n  array([0.09238525, 0.24019298, 0.33518657, 0.33223516], dtype=float32),\n  array([0.09238525, 0.24019298, 0.33518657, 0.33223516], dtype=float32)],\n 128: [array([0.6830195 , 0.23925862, 0.05575742, 0.02196444], dtype=float32),\n  array([0.6792589 , 0.23785019, 0.0600692 , 0.02282177], dtype=float32),\n  array([0.03020452, 0.22590092, 0.3841882 , 0.3597063 ], dtype=float32),\n  array([0.03020452, 0.22590092, 0.3841882 , 0.3597063 ], dtype=float32)],\n 129: [array([0.5681693 , 0.2820069 , 0.12632385, 0.0235    ], dtype=float32),\n  array([0.5671531 , 0.27399373, 0.13396583, 0.02488737], dtype=float32),\n  array([0.01345391, 0.16343756, 0.36722124, 0.45588726], dtype=float32),\n  array([0.01345391, 0.16343756, 0.36722124, 0.45588726], dtype=float32)],\n 130: [array([0.5822871 , 0.28812483, 0.10100742, 0.02858068], dtype=float32),\n  array([0.54700464, 0.2516966 , 0.1391251 , 0.06217362], dtype=float32),\n  array([0.03925711, 0.2149193 , 0.2460404 , 0.49978325], dtype=float32),\n  array([0.03925711, 0.2149193 , 0.2460404 , 0.49978325], dtype=float32)],\n 131: [array([0.7813012 , 0.1073188 , 0.09071658, 0.02066347], dtype=float32),\n  array([0.72205055, 0.11827125, 0.11913005, 0.04054809], dtype=float32),\n  array([0.23293321, 0.17427848, 0.2856489 , 0.30713943], dtype=float32),\n  array([0.23293321, 0.17427848, 0.2856489 , 0.30713943], dtype=float32)],\n 132: [array([0.4662302 , 0.34575027, 0.16825584, 0.01976371], dtype=float32),\n  array([0.5024995 , 0.30910346, 0.16620994, 0.02218716], dtype=float32),\n  array([0.07238875, 0.15456603, 0.3561997 , 0.41684547], dtype=float32),\n  array([0.07238875, 0.15456603, 0.3561997 , 0.41684547], dtype=float32)],\n 133: [array([0.6504168 , 0.2167243 , 0.10806916, 0.02478983], dtype=float32),\n  array([0.69294435, 0.15883781, 0.10998042, 0.0382375 ], dtype=float32),\n  array([0.04892913, 0.12469289, 0.469411  , 0.356967  ], dtype=float32),\n  array([0.04892913, 0.12469289, 0.469411  , 0.356967  ], dtype=float32)],\n 134: [array([0.79242045, 0.16425036, 0.03755655, 0.00577256], dtype=float32),\n  array([0.77159065, 0.15914665, 0.05492916, 0.01433353], dtype=float32),\n  array([0.02831902, 0.19503535, 0.2867085 , 0.48993716], dtype=float32),\n  array([0.02831902, 0.19503535, 0.2867085 , 0.48993716], dtype=float32)],\n 135: [array([0.7800942 , 0.14427447, 0.06209655, 0.0135349 ], dtype=float32),\n  array([0.64735955, 0.14711402, 0.13260677, 0.07291961], dtype=float32),\n  array([0.0529352 , 0.15668163, 0.3485899 , 0.44179326], dtype=float32),\n  array([0.0529352 , 0.15668163, 0.3485899 , 0.44179326], dtype=float32)],\n 136: [array([0.6671151 , 0.28380018, 0.04056692, 0.00851784], dtype=float32),\n  array([0.5907488 , 0.21566947, 0.1246672 , 0.06891447], dtype=float32),\n  array([0.03083351, 0.33692732, 0.36089894, 0.27134016], dtype=float32),\n  array([0.03083351, 0.33692732, 0.36089894, 0.27134016], dtype=float32)],\n 137: [array([0.62225485, 0.30403084, 0.06525446, 0.00845985], dtype=float32),\n  array([0.61895496, 0.23887855, 0.10653198, 0.03563451], dtype=float32),\n  array([0.05278254, 0.28060612, 0.27105856, 0.39555278], dtype=float32),\n  array([0.05278254, 0.28060612, 0.27105856, 0.39555278], dtype=float32)],\n 138: [array([0.44224662, 0.29945382, 0.11554714, 0.14275241], dtype=float32),\n  array([0.0766793 , 0.24451198, 0.20448467, 0.47432408], dtype=float32),\n  array([0.01707533, 0.21309794, 0.17939678, 0.5904299 ], dtype=float32),\n  array([0.01707533, 0.21309794, 0.17939678, 0.5904299 ], dtype=float32)],\n 139: [array([0.6758496 , 0.23464097, 0.0496422 , 0.03986725], dtype=float32),\n  array([0.6013623 , 0.1934703 , 0.10269958, 0.10246782], dtype=float32),\n  array([0.04890332, 0.24044678, 0.3079809 , 0.402669  ], dtype=float32),\n  array([0.04890332, 0.24044678, 0.3079809 , 0.402669  ], dtype=float32)],\n 140: [array([0.43422064, 0.3154029 , 0.16125311, 0.08912331], dtype=float32),\n  array([0.46299294, 0.31117755, 0.1449753 , 0.0808542 ], dtype=float32),\n  array([0.01941862, 0.22940737, 0.44140074, 0.3097733 ], dtype=float32),\n  array([0.01941862, 0.22940737, 0.44140074, 0.3097733 ], dtype=float32)],\n 141: [array([0.6314193 , 0.16499735, 0.12860808, 0.07497521], dtype=float32),\n  array([0.61027163, 0.15277977, 0.13821976, 0.09872882], dtype=float32),\n  array([0.05987664, 0.12476862, 0.21983163, 0.5955231 ], dtype=float32),\n  array([0.05980094, 0.12473356, 0.21980152, 0.595664  ], dtype=float32)],\n 142: [array([0.7519086 , 0.15876675, 0.07170346, 0.0176212 ], dtype=float32),\n  array([0.6808942 , 0.16017509, 0.11232263, 0.0466081 ], dtype=float32),\n  array([0.16964775, 0.20074749, 0.31114694, 0.31845778], dtype=float32),\n  array([0.16554989, 0.20037788, 0.31244433, 0.32162786], dtype=float32)],\n 143: [array([0.66472864, 0.24058168, 0.08638988, 0.00829982], dtype=float32),\n  array([0.63255817, 0.23154888, 0.11581864, 0.02007438], dtype=float32),\n  array([0.10278545, 0.31402117, 0.34292436, 0.24026906], dtype=float32),\n  array([0.10278545, 0.31402117, 0.34292436, 0.24026906], dtype=float32)],\n 144: [array([0.7597594 , 0.15688233, 0.07423999, 0.0091183 ], dtype=float32),\n  array([0.7520408 , 0.15399852, 0.08093569, 0.01302503], dtype=float32),\n  array([0.09857127, 0.18558659, 0.34388667, 0.37195548], dtype=float32),\n  array([0.09857127, 0.18558659, 0.34388667, 0.37195548], dtype=float32)],\n 145: [array([0.63471407, 0.29686093, 0.06574629, 0.00267872], dtype=float32),\n  array([0.6333568 , 0.30597457, 0.05825981, 0.00240872], dtype=float32),\n  array([0.18584968, 0.33305177, 0.32823747, 0.15286109], dtype=float32),\n  array([0.10920163, 0.33795294, 0.36604193, 0.18680345], dtype=float32)],\n 146: [array([0.6416794 , 0.2241122 , 0.12638614, 0.00782218], dtype=float32),\n  array([0.65572345, 0.22657356, 0.11078059, 0.00692233], dtype=float32),\n  array([0.04892477, 0.22389361, 0.36849555, 0.35868606], dtype=float32),\n  array([0.04893996, 0.22388417, 0.3684777 , 0.3586982 ], dtype=float32)],\n 147: [array([0.61476624, 0.14014904, 0.1833853 , 0.06169941], dtype=float32),\n  array([0.58930373, 0.14596486, 0.16646065, 0.09827076], dtype=float32),\n  array([0.05568815, 0.17550938, 0.24085395, 0.52794856], dtype=float32),\n  array([0.05120156, 0.1732264 , 0.23856464, 0.53700745], dtype=float32)],\n 148: [array([0.5266347 , 0.3053672 , 0.14774805, 0.02024999], dtype=float32),\n  array([0.5420454 , 0.2935463 , 0.13053322, 0.033875  ], dtype=float32),\n  array([0.05011422, 0.23374906, 0.2314891 , 0.48464757], dtype=float32),\n  array([0.02532429, 0.22463739, 0.21349563, 0.5365427 ], dtype=float32)],\n 149: [array([0.8643718 , 0.07200689, 0.0627531 , 0.00086821], dtype=float32),\n  array([0.8417449 , 0.08939676, 0.06691124, 0.00194705], dtype=float32),\n  array([0.20798324, 0.27626836, 0.34792444, 0.16782399], dtype=float32),\n  array([0.16603525, 0.30540568, 0.37695247, 0.15160659], dtype=float32)]}"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_data = {key: [sublist for sublist in value] for key, value in all_subjects_outputs.items()}\n",
    "df_data"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-14T00:36:17.183318300Z",
     "start_time": "2024-06-14T00:36:17.061529400Z"
    }
   },
   "id": "9f3699aa89de06df",
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "df_data = {key: [sublist for sublist in value] for key, value in all_subjects_outputs.items()}\n",
    "df = pd.DataFrame.from_dict(df_data, orient='index')\n",
    "df.to_excel(\"outputs_bc.xlsx\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-14T00:36:25.261160100Z",
     "start_time": "2024-06-14T00:36:25.237087400Z"
    }
   },
   "id": "e9882594a349201d",
   "execution_count": 10
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "flattened_data = {key: sublist.flatten() for key, sublist in all_subjects_outputs.items()}\n",
    "df = pd.DataFrame.from_dict(flattened_data, orient='index')\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-14T00:37:48.699385100Z",
     "start_time": "2024-06-14T00:37:47.972897100Z"
    }
   },
   "id": "b973860bae165b11",
   "execution_count": 15
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "df.to_excel(\"outputs_bc.xlsx\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-14T00:37:56.928334500Z",
     "start_time": "2024-06-14T00:37:56.657272500Z"
    }
   },
   "id": "9e55040db2ebd5d8",
   "execution_count": 17
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for Subject 0\n",
      "Training for Subject 1\n",
      "Training for Subject 2\n",
      "Training for Subject 3\n",
      "Training for Subject 4\n",
      "Training for Subject 5\n",
      "Training for Subject 6\n",
      "Training for Subject 7\n",
      "Training for Subject 8\n",
      "Training for Subject 9\n"
     ]
    }
   ],
   "source": [
    "train_len = 36\n",
    "test_len = 9\n",
    "subject_losses = []\n",
    "for subject, data_splits in cross_val_data.items():\n",
    "    if subject == 10:\n",
    "        break\n",
    "    print(f\"Training for Subject {subject}\")\n",
    "    epoch_train_losses = []\n",
    "    epoch_test_losses = []\n",
    "    \n",
    "    for fold, ((train_states, train_actions), (test_states, test_actions)) in enumerate(data_splits):\n",
    "        model = BCModel()\n",
    "        optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-5)\n",
    "        \n",
    "        epoch_train_losses_fold = []\n",
    "        epoch_test_losses_fold = []\n",
    "        \n",
    "        train_states_expanded = np.column_stack((train_states, np.roll(train_states, 1)))\n",
    "        train_states_expanded[0, 1] = train_states_expanded[0, 0]\n",
    "        \n",
    "        test_states_expanded = np.column_stack((test_states, np.roll(test_states, 1)))\n",
    "        test_states_expanded[0, 1] = test_states_expanded[0, 0]\n",
    "\n",
    "        train_states_tensor = torch.tensor(train_states_expanded, dtype=torch.float32)\n",
    "        train_actions_tensor = torch.tensor(train_actions, dtype=torch.long)\n",
    "        test_states_tensor = torch.tensor(test_states_expanded, dtype=torch.float32)\n",
    "        test_actions_tensor = torch.tensor(test_actions, dtype=torch.long)\n",
    "\n",
    "        train_dataset = TensorDataset(train_states_tensor, train_actions_tensor)\n",
    "        train_loader = DataLoader(train_dataset, batch_size=10, shuffle=True)\n",
    "        test_dataset = TensorDataset(test_states_tensor, test_actions_tensor)\n",
    "        test_loader = DataLoader(test_dataset, batch_size=10, shuffle=False)\n",
    "\n",
    "        model.train()\n",
    "        for epoch in range(75):\n",
    "            running_train_loss = 0.0\n",
    "            for inputs, labels in train_loader:\n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(inputs)\n",
    "                loss = -torch.sum(torch.log(outputs.gather(1, labels.unsqueeze(1)).squeeze(1)))\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                running_train_loss += loss.item()\n",
    "            epoch_train_losses_fold.append(running_train_loss / train_len)\n",
    "\n",
    "            # Evaluate on test set for each epoch\n",
    "            model.eval()\n",
    "            running_test_loss = 0.0\n",
    "            with torch.no_grad():\n",
    "                for inputs, labels in test_loader:\n",
    "                    outputs = model(inputs)\n",
    "                    test_loss = -torch.sum(torch.log(outputs.gather(1, labels.unsqueeze(1)).squeeze(1)))\n",
    "                    running_test_loss += test_loss.item()\n",
    "            epoch_test_losses_fold.append(running_test_loss / test_len)\n",
    "        epoch_train_losses.append(epoch_train_losses_fold)\n",
    "        epoch_test_losses.append(epoch_test_losses_fold)\n",
    "    # Plotting\n",
    "    subject_losses.append([epoch_train_losses,epoch_test_losses])\n",
    "    \n",
    "    "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-13T13:34:11.054431300Z",
     "start_time": "2024-06-13T13:33:35.224793100Z"
    }
   },
   "id": "e3505bcbe9eef44d",
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "train_losses = np.mean(np.array(subject_losses[0][0]), axis=0).tolist()\n",
    "test_losses = np.mean(np.array(subject_losses[0][1]), axis=0).tolist()\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "42d0ac7bbe179da0",
   "execution_count": 13
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming subject_losses is a list of lists where each sublist contains train and test losses for a subject\n",
    "# For example, subject_losses[0][0] contains train losses for subject 0, subject_losses[0][1] contains test losses for subject 0\n",
    "\n",
    "num_subjects = 10\n",
    "\n",
    "for i in range(num_subjects):\n",
    "    print(f'Subject {i}')\n",
    "    train_losses = np.mean(np.array(subject_losses[i][0]), axis=0)\n",
    "    test_losses = np.mean(np.array(subject_losses[i][1]), axis=0)\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(train_losses, label='Train Loss', color='blue')\n",
    "    plt.plot(test_losses, label='Test Loss', color='red')\n",
    "    \n",
    "    plt.title(f'Subject {i} Train and Test Losses')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend(title='Legend')\n",
    "    plt.grid(True)\n",
    "    \n",
    "    # Set the background color to white\n",
    "    plt.gca().set_facecolor('white')\n",
    "    plt.gcf().set_facecolor('white')\n",
    "    \n",
    "    # Set axis and text colors to black\n",
    "    plt.gca().tick_params(axis='x', colors='black')\n",
    "    plt.gca().tick_params(axis='y', colors='black')\n",
    "    plt.gca().xaxis.label.set_color('black')\n",
    "    plt.gca().yaxis.label.set_color('black')\n",
    "    plt.gca().title.set_color('black')\n",
    "    plt.gca().legend().get_title().set_color('black')\n",
    "    plt.gca().legend().get_texts()[0].set_color('black')\n",
    "    plt.gca().legend().get_texts()[1].set_color('black')\n",
    "    \n",
    "    plt.show()\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6449042dc61415d1",
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_losses' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[1], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m \u001B[43mtrain_losses\u001B[49m\n",
      "\u001B[1;31mNameError\u001B[0m: name 'train_losses' is not defined"
     ]
    }
   ],
   "source": [
    "train_losses"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-13T13:32:59.559054400Z",
     "start_time": "2024-06-13T13:32:57.931501Z"
    }
   },
   "id": "f1f1bd87c1f5c9b6",
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "bf061753a3886a4"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
